{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "matplotlib.use('Agg')   # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "images_dir = \"images\"\n",
    "\n",
    "# Training parameters\n",
    "img_size = (32, 32)  # Resize images to 64x64\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "k_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "# Dictionary to store accuracies\n",
    "cross_val_accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a simple CNN model\n",
    "def cnn_model(input_shape, num_classes):\n",
    "\tmodel = Sequential([\n",
    "\t\tConv2D(32, (5,5), activation='relu', input_shape=input_shape),\n",
    "\t\tMaxPooling2D(2,2),\n",
    "\t\tConv2D(64, (5,5), activation='relu'),\n",
    "\t\tMaxPooling2D(2,2),\n",
    "\t\tFlatten(),\n",
    "\t\tDense(128, activation='relu'),\n",
    "\t\tDense(num_classes, activation='softmax')  # Adjust output neurons based on classes\n",
    "\t])\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validating 4px/Chargaff-Composante-Diversite...\n",
      "Found 6771 images belonging to 5 classes.\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      " 64/170 [==========>...................] - ETA: 4s - loss: 0.9879 - accuracy: 0.6561"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m\n\u001b[0;32m     37\u001b[0m val_data \u001b[38;5;241m=\u001b[39m datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[0;32m     38\u001b[0m \tmethod_path, target_size\u001b[38;5;241m=\u001b[39mimg_size, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     39\u001b[0m \tclass_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Store best validation accuracy for this fold\u001b[39;00m\n\u001b[0;32m     46\u001b[0m max_fold_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through resolution folders\n",
    "for resolution_folder in sorted(os.listdir(images_dir), key=lambda x: int(re.search(r'\\d+', x).group())):\n",
    "\tresolution_path = os.path.join(images_dir, resolution_folder)\n",
    "\n",
    "\tif os.path.isdir(resolution_path):\n",
    "\t\t# Iterate through method folders inside resolution\n",
    "\t\tfor method_folder in os.listdir(resolution_path):\n",
    "\t\t\tmethod_path = os.path.join(resolution_path, method_folder)\n",
    "\n",
    "\t\t\tif os.path.isdir(method_path):\n",
    "\t\t\t\tprint(f\"\\nCross-validating {resolution_folder}/{method_folder}...\")\n",
    "\n",
    "\t\t\t\t# Data Preprocessing\n",
    "\t\t\t\tdatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\t\t\t\tdataset = datagen.flow_from_directory(\n",
    "\t\t\t\t\tmethod_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\t\t\tclass_mode='categorical', shuffle=True\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t\tnum_samples = dataset.samples\n",
    "\t\t\t\tnum_classes = dataset.num_classes\n",
    "\t\t\t\tinput_shape = (img_size[0], img_size[1], 3)\n",
    "\n",
    "\t\t\t\t# K-Fold Cross-Validation\n",
    "\t\t\t\tkfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\t\t\t\tfold_accuracies = []\n",
    "\n",
    "\t\t\t\tfor train_idx, val_idx in kfold.split(np.arange(num_samples)):\n",
    "\t\t\t\t\t# Re-initialize CNN for each fold\n",
    "\t\t\t\t\tmodel = cnn_model(input_shape, num_classes)\n",
    "\n",
    "\t\t\t\t\t# Create new data generators for train/validation split\n",
    "\t\t\t\t\ttrain_data = datagen.flow_from_directory(\n",
    "\t\t\t\t\t\tmethod_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\t\t\t\tclass_mode='categorical', subset='training'\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tval_data = datagen.flow_from_directory(\n",
    "\t\t\t\t\t\tmethod_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\t\t\t\tclass_mode='categorical', subset='validation'\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\t# Train model\n",
    "\t\t\t\t\thistory = model.fit(train_data, validation_data=val_data, epochs=epochs, verbose=1)\n",
    "\n",
    "\t\t\t\t\t# Store best validation accuracy for this fold\n",
    "\t\t\t\t\tmax_fold_acc = max(history.history['val_accuracy'])\n",
    "\t\t\t\t\tfold_accuracies.append(max_fold_acc)\n",
    "\t\t\t\t\tprint(f\"Fold accuracy: {max_fold_acc:.4f}\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# save the model\n",
    "\t\t\t\tmodel_path = os.path.join(\"models/kfold\", resolution_folder, method_folder)\n",
    "\t\t\t\tos.makedirs(model_path, exist_ok=True)\n",
    "\t\t\t\tmodel_name = f\"{resolution_folder}_{method_folder}_fold_{len(fold_accuracies)}_mask_5.h5\"\n",
    "\t\t\t\tmodel.save(os.path.join(model_path, model_name))\n",
    "\t\t\t\tprint(f\"Model saved to {model_path}\")\n",
    "\n",
    "\t\t\t\t# Store average accuracy for this method-resolution combo\n",
    "\t\t\t\tavg_acc = np.mean(fold_accuracies)\n",
    "\t\t\t\tkey = f\"{resolution_folder} - {method_folder}\"\n",
    "\t\t\t\tcross_val_accuracies[key] = avg_acc\n",
    "\t\t\t\tprint(f\"Average accuracy for {key}: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\4138100061.py:21: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Create positions for bars\n",
    "num_bars = len(cross_val_accuracies)\n",
    "x_positions = np.arange(num_bars)\n",
    "\n",
    "# Add extra space every 10 bars\n",
    "for i in range(10, num_bars, 10):  \n",
    "\tx_positions[i:] += 1  # Shift everything after every 10th bar\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(x_positions, cross_val_accuracies.values(), color='blue', width=0.5)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Custom x-ticks for readability\n",
    "plt.xticks(x_positions, cross_val_accuracies.keys(), rotation=90)\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"CNN Validation Results\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\3514672204.py:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap(\"tab10\", len(methods))  # Use \"tab10\" color map with enough colors\n",
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\3514672204.py:31: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Extract and sort resolutions numerically\n",
    "resolutions = sorted(set(k.split(\" - \")[0] for k in cross_val_accuracies.keys()), key=lambda x: int(x.replace(\"px\", \"\")))\n",
    "methods = sorted(set(k.split(\" - \")[1] for k in cross_val_accuracies.keys()))\n",
    "\n",
    "# Organize data for plotting\n",
    "data = {method: [cross_val_accuracies.get(f\"{res} - {method}\", None) for res in resolutions] for method in methods}\n",
    "\n",
    "# Define a color map for better distinction\n",
    "colors = plt.cm.get_cmap(\"tab10\", len(methods))  # Use \"tab10\" color map with enough colors\n",
    "\n",
    "# Plot the lines\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (method, accuracies) in enumerate(data.items()):\n",
    "\tplt.plot(resolutions, accuracies, marker=\"o\", linestyle=\"-\", linewidth=2, markersize=8, label=method, color=colors(i))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Resolution\", fontsize=12)\n",
    "plt.ylabel(\"Max Accuracy\", fontsize=12)\n",
    "plt.title(\"CNN Validation Accuracy by Resolution and Method\", fontsize=14)\n",
    "\n",
    "# Move legend outside the plot for better clarity\n",
    "plt.legend(title=\"Method\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=10)\n",
    "\n",
    "# Add a grid with transparency\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Improve layout to fit legend properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/kfold/cnn_validation_accuracy_mask_5-kfold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\1168393777.py:16: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap(\"tab10\", len(methods_group1))  # Use \"tab10\" color map with enough colors\n",
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\1168393777.py:37: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Extract and sort resolutions numerically\n",
    "resolutions = sorted(set(k.split(\" - \")[0] for k in cross_val_accuracies.keys()), key=lambda x: int(x.replace(\"px\", \"\")))\n",
    "\n",
    "# Extract unique methods and split into two groups of 5\n",
    "methods = sorted(set(k.split(\" - \")[1] for k in cross_val_accuracies.keys()))\n",
    "methods_group1 = methods[:5]\n",
    "methods_group2 = methods[5:]\n",
    "\n",
    "# Organize data for plotting\n",
    "data1 = {method: [cross_val_accuracies.get(f\"{res} - {method}\", None) for res in resolutions] for method in methods_group1}\n",
    "data2 = {method: [cross_val_accuracies.get(f\"{res} - {method}\", None) for res in resolutions] for method in methods_group2}\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "colors = plt.cm.get_cmap(\"tab10\", len(methods_group1))  # Use \"tab10\" color map with enough colors\n",
    "\n",
    "# Plot first group\n",
    "for method, accuracies in data1.items():\n",
    "\taxs[0].plot(resolutions, accuracies, marker=\"o\", label=method, color=colors(methods_group1.index(method)))\n",
    "axs[0].set_title(\"CNN Validation Accuracy (Group 1)\")\n",
    "axs[0].set_ylabel(\"Max Accuracy\")\n",
    "axs[0].legend(title=\"Method\")\n",
    "axs[0].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Plot second group\n",
    "for method, accuracies in data2.items():\n",
    "\taxs[1].plot(resolutions, accuracies, marker=\"o\", label=method, color=colors(methods_group2.index(method)))\n",
    "axs[1].set_title(\"CNN Validation Accuracy (Group 2)\")\n",
    "axs[1].set_xlabel(\"Resolution\")\n",
    "axs[1].set_ylabel(\"Max Accuracy\")\n",
    "axs[1].legend(title=\"Method\")\n",
    "axs[1].grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/kfold/cnn_validation_accuracy_groups_mask_5_kfold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_249\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_498 (Conv2D)         (None, 28, 28, 32)        2432      \n",
      "                                                                 \n",
      " max_pooling2d_498 (MaxPool  (None, 14, 14, 32)        0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " conv2d_499 (Conv2D)         (None, 10, 10, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_499 (MaxPool  (None, 5, 5, 64)          0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " flatten_249 (Flatten)       (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_498 (Dense)           (None, 128)               204928    \n",
      "                                                                 \n",
      " dense_499 (Dense)           (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 259269 (1012.77 KB)\n",
      "Trainable params: 259269 (1012.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Found 6771 images belonging to 5 classes.\n",
      "212/212 [==============================] - 3s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\1061851448.py:38: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model_path = \"models/kfold/100px/Skew-Diversite-NucleScore/100px_Skew-Diversite-NucleScore_fold_5_mask_5.h5\"\n",
    "model = load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# Load the test data\n",
    "test_data_dir = 'images/100px/Chargaff-Composante-Diversite'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "\ttest_data_dir,\n",
    "\ttarget_size=(img_size[0], img_size[1]),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='categorical',\n",
    "\tshuffle=False\n",
    ")\n",
    "# Get the true labels\n",
    "true_labels = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Get the predicted labels\n",
    "predictions = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs//kfold-undersample/cnn_confusion_matrix_100px_mask_5-kfold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "mosaics_dir = \"C:/Users/theof/OneDrive/Documents/Github/genome_color_unpickler/res\"\n",
    "\n",
    "# Training parameters\n",
    "img_size = (20, 50)  # Resize images to 64x64\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "k_folds = 5  # Number of folds for cross-validation\n",
    "\n",
    "# Dictionary to store max accuracies\n",
    "cross_val_accuracies_mos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validating 4px...\n",
      "Found 6771 images belonging to 5 classes.\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 6s 30ms/step - loss: 0.9596 - accuracy: 0.6557 - val_loss: 0.9470 - val_accuracy: 0.6368\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.9037 - accuracy: 0.6741 - val_loss: 0.9454 - val_accuracy: 0.6428\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8800 - accuracy: 0.6839 - val_loss: 0.9218 - val_accuracy: 0.6649\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8697 - accuracy: 0.6915 - val_loss: 0.9383 - val_accuracy: 0.6509\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8561 - accuracy: 0.6996 - val_loss: 0.8886 - val_accuracy: 0.6627\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8269 - accuracy: 0.7110 - val_loss: 0.8354 - val_accuracy: 0.6953\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8034 - accuracy: 0.7269 - val_loss: 0.8484 - val_accuracy: 0.7034\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.7892 - accuracy: 0.7378 - val_loss: 0.7748 - val_accuracy: 0.7308\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7687 - accuracy: 0.7405 - val_loss: 0.7632 - val_accuracy: 0.7522\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.7588 - accuracy: 0.7494 - val_loss: 0.7540 - val_accuracy: 0.7470\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.7370 - accuracy: 0.7603 - val_loss: 0.7451 - val_accuracy: 0.7507\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7385 - accuracy: 0.7568 - val_loss: 0.7213 - val_accuracy: 0.7567\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7217 - accuracy: 0.7671 - val_loss: 0.7194 - val_accuracy: 0.7567\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7087 - accuracy: 0.7664 - val_loss: 0.7203 - val_accuracy: 0.7478\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7110 - accuracy: 0.7671 - val_loss: 0.7106 - val_accuracy: 0.7552\n",
      "Fold accuracy: 0.7567\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 19ms/step - loss: 0.9573 - accuracy: 0.6558 - val_loss: 0.9500 - val_accuracy: 0.6494\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8958 - accuracy: 0.6796 - val_loss: 0.9253 - val_accuracy: 0.6383\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.8897 - accuracy: 0.6734 - val_loss: 0.9320 - val_accuracy: 0.6575\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.8731 - accuracy: 0.6920 - val_loss: 0.9117 - val_accuracy: 0.6531\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8537 - accuracy: 0.6999 - val_loss: 0.8884 - val_accuracy: 0.6783\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8485 - accuracy: 0.7009 - val_loss: 0.8914 - val_accuracy: 0.6805\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.8377 - accuracy: 0.7071 - val_loss: 0.8743 - val_accuracy: 0.6731\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8215 - accuracy: 0.7160 - val_loss: 0.8529 - val_accuracy: 0.6857\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.8067 - accuracy: 0.7249 - val_loss: 0.8492 - val_accuracy: 0.6916\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 4s 25ms/step - loss: 0.8002 - accuracy: 0.7285 - val_loss: 0.8882 - val_accuracy: 0.6679\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.7719 - accuracy: 0.7380 - val_loss: 0.7719 - val_accuracy: 0.7367\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7596 - accuracy: 0.7476 - val_loss: 0.7650 - val_accuracy: 0.7241\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.7423 - accuracy: 0.7522 - val_loss: 0.7417 - val_accuracy: 0.7426\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 4s 24ms/step - loss: 0.7390 - accuracy: 0.7595 - val_loss: 0.7246 - val_accuracy: 0.7537\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7173 - accuracy: 0.7660 - val_loss: 0.7168 - val_accuracy: 0.7655\n",
      "Fold accuracy: 0.7655\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.9610 - accuracy: 0.6560 - val_loss: 0.9507 - val_accuracy: 0.6516\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.9022 - accuracy: 0.6802 - val_loss: 0.9177 - val_accuracy: 0.6509\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.8826 - accuracy: 0.6861 - val_loss: 0.9500 - val_accuracy: 0.6531\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8607 - accuracy: 0.6981 - val_loss: 0.9630 - val_accuracy: 0.6531\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8411 - accuracy: 0.7092 - val_loss: 0.8464 - val_accuracy: 0.6990\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.8215 - accuracy: 0.7191 - val_loss: 0.8120 - val_accuracy: 0.7271\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.7877 - accuracy: 0.7352 - val_loss: 0.8069 - val_accuracy: 0.7123\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.7637 - accuracy: 0.7468 - val_loss: 0.7483 - val_accuracy: 0.7426\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.7526 - accuracy: 0.7570 - val_loss: 0.7588 - val_accuracy: 0.7515\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.7341 - accuracy: 0.7612 - val_loss: 0.7027 - val_accuracy: 0.7685\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.7206 - accuracy: 0.7712 - val_loss: 0.8639 - val_accuracy: 0.6938\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7191 - accuracy: 0.7664 - val_loss: 0.7095 - val_accuracy: 0.7515\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7349 - accuracy: 0.7572 - val_loss: 0.7141 - val_accuracy: 0.7552\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7120 - accuracy: 0.7714 - val_loss: 0.6922 - val_accuracy: 0.7722\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7132 - accuracy: 0.7671 - val_loss: 0.6877 - val_accuracy: 0.7788\n",
      "Fold accuracy: 0.7788\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.9701 - accuracy: 0.6507 - val_loss: 0.9506 - val_accuracy: 0.6280\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.9030 - accuracy: 0.6780 - val_loss: 0.9601 - val_accuracy: 0.6368\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8888 - accuracy: 0.6878 - val_loss: 0.9348 - val_accuracy: 0.6405\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8718 - accuracy: 0.6902 - val_loss: 0.9049 - val_accuracy: 0.6524\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8620 - accuracy: 0.6963 - val_loss: 0.9022 - val_accuracy: 0.6783\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8452 - accuracy: 0.7086 - val_loss: 0.9130 - val_accuracy: 0.6391\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8323 - accuracy: 0.7195 - val_loss: 0.8634 - val_accuracy: 0.6879\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8152 - accuracy: 0.7300 - val_loss: 0.8338 - val_accuracy: 0.7130\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8055 - accuracy: 0.7234 - val_loss: 0.8208 - val_accuracy: 0.7263\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7861 - accuracy: 0.7376 - val_loss: 0.8422 - val_accuracy: 0.6953\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7809 - accuracy: 0.7359 - val_loss: 0.7982 - val_accuracy: 0.7152\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7431 - accuracy: 0.7562 - val_loss: 0.7501 - val_accuracy: 0.7389\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7243 - accuracy: 0.7651 - val_loss: 0.7373 - val_accuracy: 0.7426\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7299 - accuracy: 0.7601 - val_loss: 0.7013 - val_accuracy: 0.7633\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.7134 - accuracy: 0.7664 - val_loss: 0.7052 - val_accuracy: 0.7596\n",
      "Fold accuracy: 0.7633\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.9863 - accuracy: 0.6485 - val_loss: 0.9761 - val_accuracy: 0.6501\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.9062 - accuracy: 0.6697 - val_loss: 0.9322 - val_accuracy: 0.6391\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.8833 - accuracy: 0.6782 - val_loss: 0.9216 - val_accuracy: 0.6509\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.8725 - accuracy: 0.6889 - val_loss: 0.9059 - val_accuracy: 0.6649\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8578 - accuracy: 0.6975 - val_loss: 0.8959 - val_accuracy: 0.6738\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8433 - accuracy: 0.7009 - val_loss: 0.8901 - val_accuracy: 0.6871\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.8382 - accuracy: 0.7095 - val_loss: 0.8967 - val_accuracy: 0.6738\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8370 - accuracy: 0.7084 - val_loss: 0.8631 - val_accuracy: 0.6982\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8197 - accuracy: 0.7173 - val_loss: 0.8498 - val_accuracy: 0.6923\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.8003 - accuracy: 0.7280 - val_loss: 0.8209 - val_accuracy: 0.7152\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7775 - accuracy: 0.7381 - val_loss: 0.8016 - val_accuracy: 0.7300\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7803 - accuracy: 0.7311 - val_loss: 0.8044 - val_accuracy: 0.7101\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7665 - accuracy: 0.7418 - val_loss: 0.7849 - val_accuracy: 0.7419\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7442 - accuracy: 0.7548 - val_loss: 0.7497 - val_accuracy: 0.7404\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.7422 - accuracy: 0.7507 - val_loss: 0.7347 - val_accuracy: 0.7567\n",
      "Fold accuracy: 0.7567\n",
      "Model saved to models/kfold_mosaic\\4px\n",
      "Average accuracy for 4px: 0.7642\n",
      "\n",
      "Cross-validating 16px...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6771 images belonging to 5 classes.\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 6s 29ms/step - loss: 0.8325 - accuracy: 0.7237 - val_loss: 0.8477 - val_accuracy: 0.7086\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.5628 - accuracy: 0.8171 - val_loss: 0.5423 - val_accuracy: 0.8262\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.4235 - accuracy: 0.8622 - val_loss: 0.4632 - val_accuracy: 0.8388\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.3553 - accuracy: 0.8850 - val_loss: 0.4017 - val_accuracy: 0.8632\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.3235 - accuracy: 0.8991 - val_loss: 0.4797 - val_accuracy: 0.8284\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2967 - accuracy: 0.9088 - val_loss: 0.4939 - val_accuracy: 0.8609\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2895 - accuracy: 0.9127 - val_loss: 0.3537 - val_accuracy: 0.8809\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2782 - accuracy: 0.9136 - val_loss: 0.3634 - val_accuracy: 0.8824\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2479 - accuracy: 0.9271 - val_loss: 0.3539 - val_accuracy: 0.8868\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2467 - accuracy: 0.9253 - val_loss: 0.4113 - val_accuracy: 0.8817\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2323 - accuracy: 0.9302 - val_loss: 0.3943 - val_accuracy: 0.8928\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2230 - accuracy: 0.9328 - val_loss: 0.3389 - val_accuracy: 0.8957\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2209 - accuracy: 0.9323 - val_loss: 0.4040 - val_accuracy: 0.8839\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2198 - accuracy: 0.9332 - val_loss: 0.4487 - val_accuracy: 0.8491\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.2006 - accuracy: 0.9404 - val_loss: 0.3945 - val_accuracy: 0.8824\n",
      "Fold accuracy: 0.8957\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.8191 - accuracy: 0.7191 - val_loss: 0.8313 - val_accuracy: 0.7160\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.5839 - accuracy: 0.8127 - val_loss: 0.6461 - val_accuracy: 0.7714\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.4417 - accuracy: 0.8570 - val_loss: 0.6005 - val_accuracy: 0.7892\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.3673 - accuracy: 0.8850 - val_loss: 0.3958 - val_accuracy: 0.8757\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.3158 - accuracy: 0.9031 - val_loss: 0.3887 - val_accuracy: 0.8772\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.3019 - accuracy: 0.9035 - val_loss: 0.4465 - val_accuracy: 0.8476\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2910 - accuracy: 0.9105 - val_loss: 0.3630 - val_accuracy: 0.8772\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2742 - accuracy: 0.9179 - val_loss: 0.4563 - val_accuracy: 0.8417\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2526 - accuracy: 0.9230 - val_loss: 0.3615 - val_accuracy: 0.8861\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2452 - accuracy: 0.9258 - val_loss: 0.3881 - val_accuracy: 0.8839\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2312 - accuracy: 0.9291 - val_loss: 0.3859 - val_accuracy: 0.8861\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2383 - accuracy: 0.9284 - val_loss: 0.4534 - val_accuracy: 0.8277\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2137 - accuracy: 0.9354 - val_loss: 0.4032 - val_accuracy: 0.8898\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.2138 - accuracy: 0.9380 - val_loss: 0.3982 - val_accuracy: 0.8920\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2060 - accuracy: 0.9404 - val_loss: 0.4304 - val_accuracy: 0.8935\n",
      "Fold accuracy: 0.8935\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.8360 - accuracy: 0.7202 - val_loss: 0.8165 - val_accuracy: 0.7130\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.5865 - accuracy: 0.8055 - val_loss: 0.6638 - val_accuracy: 0.7522\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.4404 - accuracy: 0.8596 - val_loss: 0.5081 - val_accuracy: 0.8380\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.3650 - accuracy: 0.8843 - val_loss: 0.4336 - val_accuracy: 0.8410\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.3328 - accuracy: 0.8965 - val_loss: 0.4108 - val_accuracy: 0.8580\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.3112 - accuracy: 0.9031 - val_loss: 0.3754 - val_accuracy: 0.8802\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2887 - accuracy: 0.9090 - val_loss: 0.3785 - val_accuracy: 0.8817\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2642 - accuracy: 0.9221 - val_loss: 0.3842 - val_accuracy: 0.8676\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2531 - accuracy: 0.9221 - val_loss: 0.3577 - val_accuracy: 0.8817\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.2451 - accuracy: 0.9230 - val_loss: 0.4222 - val_accuracy: 0.8765\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.2374 - accuracy: 0.9254 - val_loss: 0.4512 - val_accuracy: 0.8499\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2266 - accuracy: 0.9319 - val_loss: 0.3297 - val_accuracy: 0.8987\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.2241 - accuracy: 0.9341 - val_loss: 0.3821 - val_accuracy: 0.8935\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2069 - accuracy: 0.9382 - val_loss: 0.3609 - val_accuracy: 0.8920\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.2024 - accuracy: 0.9428 - val_loss: 0.3443 - val_accuracy: 0.8928\n",
      "Fold accuracy: 0.8987\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.8788 - accuracy: 0.6987 - val_loss: 0.9246 - val_accuracy: 0.7064\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.6259 - accuracy: 0.7931 - val_loss: 0.5832 - val_accuracy: 0.8092\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.4430 - accuracy: 0.8581 - val_loss: 0.4882 - val_accuracy: 0.8484\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.3848 - accuracy: 0.8780 - val_loss: 0.4891 - val_accuracy: 0.8262\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.3300 - accuracy: 0.8987 - val_loss: 0.5792 - val_accuracy: 0.8254\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.3087 - accuracy: 0.9051 - val_loss: 0.4586 - val_accuracy: 0.8365\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2875 - accuracy: 0.9131 - val_loss: 0.3763 - val_accuracy: 0.8691\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.2759 - accuracy: 0.9133 - val_loss: 0.4070 - val_accuracy: 0.8787\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2625 - accuracy: 0.9227 - val_loss: 0.3599 - val_accuracy: 0.8883\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2594 - accuracy: 0.9177 - val_loss: 0.3747 - val_accuracy: 0.8750\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.2374 - accuracy: 0.9269 - val_loss: 0.4137 - val_accuracy: 0.8661\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2322 - accuracy: 0.9288 - val_loss: 0.3633 - val_accuracy: 0.8876\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2296 - accuracy: 0.9365 - val_loss: 0.3434 - val_accuracy: 0.8920\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2175 - accuracy: 0.9354 - val_loss: 0.6903 - val_accuracy: 0.7759\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2217 - accuracy: 0.9347 - val_loss: 0.4539 - val_accuracy: 0.8432\n",
      "Fold accuracy: 0.8920\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.8170 - accuracy: 0.7212 - val_loss: 0.7735 - val_accuracy: 0.7352\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.5483 - accuracy: 0.8197 - val_loss: 0.6149 - val_accuracy: 0.7796\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.4117 - accuracy: 0.8692 - val_loss: 0.4737 - val_accuracy: 0.8506\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.3499 - accuracy: 0.8919 - val_loss: 0.3942 - val_accuracy: 0.8632\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.3097 - accuracy: 0.9013 - val_loss: 0.3887 - val_accuracy: 0.8706\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.2876 - accuracy: 0.9105 - val_loss: 0.3980 - val_accuracy: 0.8513\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2751 - accuracy: 0.9147 - val_loss: 0.4359 - val_accuracy: 0.8669\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.2596 - accuracy: 0.9205 - val_loss: 0.4269 - val_accuracy: 0.8706\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2516 - accuracy: 0.9229 - val_loss: 0.3879 - val_accuracy: 0.8905\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2355 - accuracy: 0.9273 - val_loss: 0.3835 - val_accuracy: 0.8913\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2251 - accuracy: 0.9275 - val_loss: 0.3752 - val_accuracy: 0.8809\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2179 - accuracy: 0.9347 - val_loss: 0.3983 - val_accuracy: 0.8928\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2087 - accuracy: 0.9397 - val_loss: 0.4283 - val_accuracy: 0.8898\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2114 - accuracy: 0.9360 - val_loss: 0.4823 - val_accuracy: 0.8706\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1946 - accuracy: 0.9424 - val_loss: 0.3880 - val_accuracy: 0.8972\n",
      "Fold accuracy: 0.8972\n",
      "Model saved to models/kfold_mosaic\\16px\n",
      "Average accuracy for 16px: 0.8954\n",
      "\n",
      "Cross-validating 36px...\n",
      "Found 6771 images belonging to 5 classes.\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 6s 34ms/step - loss: 0.8782 - accuracy: 0.6939 - val_loss: 0.6296 - val_accuracy: 0.8195\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.4233 - accuracy: 0.8660 - val_loss: 0.3677 - val_accuracy: 0.8891\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2705 - accuracy: 0.9192 - val_loss: 0.3355 - val_accuracy: 0.8854\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2258 - accuracy: 0.9323 - val_loss: 0.3667 - val_accuracy: 0.8802\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1831 - accuracy: 0.9483 - val_loss: 0.2764 - val_accuracy: 0.9157\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1770 - accuracy: 0.9509 - val_loss: 0.2567 - val_accuracy: 0.9172\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1696 - accuracy: 0.9528 - val_loss: 0.2949 - val_accuracy: 0.9246\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1521 - accuracy: 0.9598 - val_loss: 0.2793 - val_accuracy: 0.9327\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1452 - accuracy: 0.9618 - val_loss: 0.3506 - val_accuracy: 0.9142\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1335 - accuracy: 0.9642 - val_loss: 0.3016 - val_accuracy: 0.9186\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1292 - accuracy: 0.9662 - val_loss: 0.3703 - val_accuracy: 0.9172\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1252 - accuracy: 0.9686 - val_loss: 0.3625 - val_accuracy: 0.9157\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1245 - accuracy: 0.9679 - val_loss: 0.3516 - val_accuracy: 0.9216\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.1233 - accuracy: 0.9677 - val_loss: 0.4054 - val_accuracy: 0.8891\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1229 - accuracy: 0.9675 - val_loss: 0.3220 - val_accuracy: 0.9246\n",
      "Fold accuracy: 0.9327\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 19ms/step - loss: 0.8102 - accuracy: 0.7282 - val_loss: 0.5951 - val_accuracy: 0.7929\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.3733 - accuracy: 0.8856 - val_loss: 0.3196 - val_accuracy: 0.8957\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2681 - accuracy: 0.9151 - val_loss: 0.3321 - val_accuracy: 0.9031\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2195 - accuracy: 0.9338 - val_loss: 0.3011 - val_accuracy: 0.8950\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1999 - accuracy: 0.9408 - val_loss: 0.3116 - val_accuracy: 0.9061\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1755 - accuracy: 0.9513 - val_loss: 0.3493 - val_accuracy: 0.9068\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1561 - accuracy: 0.9564 - val_loss: 0.3166 - val_accuracy: 0.9216\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1515 - accuracy: 0.9622 - val_loss: 0.3162 - val_accuracy: 0.9038\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1420 - accuracy: 0.9638 - val_loss: 0.3095 - val_accuracy: 0.9157\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1436 - accuracy: 0.9622 - val_loss: 0.3133 - val_accuracy: 0.9201\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1301 - accuracy: 0.9659 - val_loss: 0.2895 - val_accuracy: 0.9142\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 21ms/step - loss: 0.1356 - accuracy: 0.9636 - val_loss: 0.4129 - val_accuracy: 0.9157\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1272 - accuracy: 0.9655 - val_loss: 0.2607 - val_accuracy: 0.9283\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1167 - accuracy: 0.9701 - val_loss: 0.3031 - val_accuracy: 0.9260\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1213 - accuracy: 0.9666 - val_loss: 0.2823 - val_accuracy: 0.9312\n",
      "Fold accuracy: 0.9312\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.8422 - accuracy: 0.7175 - val_loss: 0.6152 - val_accuracy: 0.8550\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.3931 - accuracy: 0.8777 - val_loss: 0.3550 - val_accuracy: 0.8868\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2733 - accuracy: 0.9155 - val_loss: 0.3092 - val_accuracy: 0.8964\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2226 - accuracy: 0.9343 - val_loss: 0.3385 - val_accuracy: 0.8913\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.2034 - accuracy: 0.9369 - val_loss: 0.3187 - val_accuracy: 0.9046\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1822 - accuracy: 0.9478 - val_loss: 0.3514 - val_accuracy: 0.9120\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1646 - accuracy: 0.9548 - val_loss: 0.2759 - val_accuracy: 0.9179\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1506 - accuracy: 0.9592 - val_loss: 0.3123 - val_accuracy: 0.9194\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1366 - accuracy: 0.9673 - val_loss: 0.3227 - val_accuracy: 0.9149\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1374 - accuracy: 0.9646 - val_loss: 0.3273 - val_accuracy: 0.9238\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1324 - accuracy: 0.9644 - val_loss: 0.3047 - val_accuracy: 0.9186\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1336 - accuracy: 0.9649 - val_loss: 0.3151 - val_accuracy: 0.9179\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1374 - accuracy: 0.9625 - val_loss: 0.3001 - val_accuracy: 0.9283\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1266 - accuracy: 0.9651 - val_loss: 0.3270 - val_accuracy: 0.9179\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1155 - accuracy: 0.9690 - val_loss: 0.3480 - val_accuracy: 0.9186\n",
      "Fold accuracy: 0.9283\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.8200 - accuracy: 0.7182 - val_loss: 0.5743 - val_accuracy: 0.8728\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.4041 - accuracy: 0.8760 - val_loss: 0.3268 - val_accuracy: 0.8928\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2904 - accuracy: 0.9107 - val_loss: 0.3346 - val_accuracy: 0.8987\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2374 - accuracy: 0.9299 - val_loss: 0.3435 - val_accuracy: 0.9009\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1933 - accuracy: 0.9474 - val_loss: 0.3242 - val_accuracy: 0.9246\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1870 - accuracy: 0.9498 - val_loss: 0.3402 - val_accuracy: 0.9268\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1632 - accuracy: 0.9542 - val_loss: 0.3155 - val_accuracy: 0.9231\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1562 - accuracy: 0.9600 - val_loss: 0.2942 - val_accuracy: 0.9275\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.1514 - accuracy: 0.9607 - val_loss: 0.2920 - val_accuracy: 0.9253\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1379 - accuracy: 0.9642 - val_loss: 0.3312 - val_accuracy: 0.9172\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.1306 - accuracy: 0.9670 - val_loss: 0.3409 - val_accuracy: 0.9290\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1371 - accuracy: 0.9648 - val_loss: 0.3883 - val_accuracy: 0.9223\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.1295 - accuracy: 0.9666 - val_loss: 0.2939 - val_accuracy: 0.9275\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.1111 - accuracy: 0.9703 - val_loss: 0.2864 - val_accuracy: 0.9334\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.1244 - accuracy: 0.9648 - val_loss: 0.3916 - val_accuracy: 0.9142\n",
      "Fold accuracy: 0.9334\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 5s 23ms/step - loss: 0.8268 - accuracy: 0.7136 - val_loss: 0.7624 - val_accuracy: 0.7027\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.3982 - accuracy: 0.8762 - val_loss: 0.3638 - val_accuracy: 0.8743\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2760 - accuracy: 0.9175 - val_loss: 0.3824 - val_accuracy: 0.8765\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.2155 - accuracy: 0.9363 - val_loss: 0.2937 - val_accuracy: 0.8987\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 4s 25ms/step - loss: 0.1955 - accuracy: 0.9424 - val_loss: 0.2784 - val_accuracy: 0.9164\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1736 - accuracy: 0.9513 - val_loss: 0.3027 - val_accuracy: 0.9209\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1578 - accuracy: 0.9588 - val_loss: 0.3585 - val_accuracy: 0.9053\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1434 - accuracy: 0.9659 - val_loss: 0.3018 - val_accuracy: 0.9209\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1384 - accuracy: 0.9636 - val_loss: 0.2923 - val_accuracy: 0.9231\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1283 - accuracy: 0.9686 - val_loss: 0.2710 - val_accuracy: 0.9283\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1274 - accuracy: 0.9657 - val_loss: 0.2856 - val_accuracy: 0.9283\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1303 - accuracy: 0.9642 - val_loss: 0.2658 - val_accuracy: 0.9297\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1187 - accuracy: 0.9681 - val_loss: 0.3103 - val_accuracy: 0.9297\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1198 - accuracy: 0.9699 - val_loss: 0.3390 - val_accuracy: 0.9246\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1138 - accuracy: 0.9694 - val_loss: 0.3080 - val_accuracy: 0.9201\n",
      "Fold accuracy: 0.9297\n",
      "Model saved to models/kfold_mosaic\\36px\n",
      "Average accuracy for 36px: 0.9311\n",
      "\n",
      "Cross-validating 64px...\n",
      "Found 6771 images belonging to 5 classes.\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 6s 34ms/step - loss: 0.6919 - accuracy: 0.7789 - val_loss: 0.5404 - val_accuracy: 0.8217\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.3609 - accuracy: 0.8817 - val_loss: 0.3978 - val_accuracy: 0.8913\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2398 - accuracy: 0.9334 - val_loss: 0.3038 - val_accuracy: 0.9046\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1809 - accuracy: 0.9557 - val_loss: 0.3334 - val_accuracy: 0.8950\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1556 - accuracy: 0.9605 - val_loss: 0.3012 - val_accuracy: 0.9083\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1385 - accuracy: 0.9651 - val_loss: 0.2636 - val_accuracy: 0.9223\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1334 - accuracy: 0.9646 - val_loss: 0.2842 - val_accuracy: 0.9149\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1214 - accuracy: 0.9688 - val_loss: 0.2843 - val_accuracy: 0.9201\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1188 - accuracy: 0.9705 - val_loss: 0.4117 - val_accuracy: 0.9068\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1059 - accuracy: 0.9734 - val_loss: 0.4101 - val_accuracy: 0.9098\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1030 - accuracy: 0.9736 - val_loss: 0.3777 - val_accuracy: 0.9038\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1016 - accuracy: 0.9745 - val_loss: 0.2766 - val_accuracy: 0.9223\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0989 - accuracy: 0.9745 - val_loss: 0.3282 - val_accuracy: 0.9186\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0937 - accuracy: 0.9764 - val_loss: 0.2686 - val_accuracy: 0.9201\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0899 - accuracy: 0.9762 - val_loss: 0.4019 - val_accuracy: 0.8972\n",
      "Fold accuracy: 0.9223\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.7509 - accuracy: 0.7586 - val_loss: 0.6255 - val_accuracy: 0.8055\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.4115 - accuracy: 0.8657 - val_loss: 0.4749 - val_accuracy: 0.8513\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2902 - accuracy: 0.9083 - val_loss: 0.3868 - val_accuracy: 0.8957\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2207 - accuracy: 0.9376 - val_loss: 0.3101 - val_accuracy: 0.9075\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1734 - accuracy: 0.9531 - val_loss: 0.3277 - val_accuracy: 0.9075\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1386 - accuracy: 0.9648 - val_loss: 0.2980 - val_accuracy: 0.9149\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1340 - accuracy: 0.9660 - val_loss: 0.2904 - val_accuracy: 0.9209\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1262 - accuracy: 0.9690 - val_loss: 0.3229 - val_accuracy: 0.9157\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1236 - accuracy: 0.9692 - val_loss: 0.3565 - val_accuracy: 0.9157\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1201 - accuracy: 0.9694 - val_loss: 0.3231 - val_accuracy: 0.9209\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1029 - accuracy: 0.9749 - val_loss: 0.2903 - val_accuracy: 0.9275\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1107 - accuracy: 0.9712 - val_loss: 0.2770 - val_accuracy: 0.9179\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1019 - accuracy: 0.9753 - val_loss: 0.3328 - val_accuracy: 0.9172\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0957 - accuracy: 0.9777 - val_loss: 0.3361 - val_accuracy: 0.9275\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0989 - accuracy: 0.9732 - val_loss: 0.3705 - val_accuracy: 0.9186\n",
      "Fold accuracy: 0.9275\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.7158 - accuracy: 0.7727 - val_loss: 0.5774 - val_accuracy: 0.8136\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.3839 - accuracy: 0.8775 - val_loss: 0.4101 - val_accuracy: 0.8765\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2474 - accuracy: 0.9264 - val_loss: 0.3213 - val_accuracy: 0.9090\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1903 - accuracy: 0.9524 - val_loss: 0.3029 - val_accuracy: 0.9209\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1649 - accuracy: 0.9576 - val_loss: 0.2864 - val_accuracy: 0.9090\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1488 - accuracy: 0.9612 - val_loss: 0.3109 - val_accuracy: 0.9157\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1335 - accuracy: 0.9681 - val_loss: 0.3126 - val_accuracy: 0.9142\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1199 - accuracy: 0.9712 - val_loss: 0.2641 - val_accuracy: 0.9275\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1144 - accuracy: 0.9708 - val_loss: 0.2904 - val_accuracy: 0.9231\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1081 - accuracy: 0.9745 - val_loss: 0.3318 - val_accuracy: 0.9149\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1024 - accuracy: 0.9742 - val_loss: 0.3605 - val_accuracy: 0.9038\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1049 - accuracy: 0.9743 - val_loss: 0.3290 - val_accuracy: 0.9223\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0991 - accuracy: 0.9743 - val_loss: 0.3329 - val_accuracy: 0.9149\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1015 - accuracy: 0.9734 - val_loss: 0.3170 - val_accuracy: 0.9194\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0925 - accuracy: 0.9756 - val_loss: 0.3123 - val_accuracy: 0.9179\n",
      "Fold accuracy: 0.9275\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.6922 - accuracy: 0.7839 - val_loss: 0.5445 - val_accuracy: 0.8306\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.3424 - accuracy: 0.8876 - val_loss: 0.3712 - val_accuracy: 0.8868\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2158 - accuracy: 0.9421 - val_loss: 0.3116 - val_accuracy: 0.9083\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1777 - accuracy: 0.9526 - val_loss: 0.3151 - val_accuracy: 0.9068\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1497 - accuracy: 0.9607 - val_loss: 0.3236 - val_accuracy: 0.9142\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1357 - accuracy: 0.9653 - val_loss: 0.3101 - val_accuracy: 0.9135\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1289 - accuracy: 0.9653 - val_loss: 0.3191 - val_accuracy: 0.9090\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1170 - accuracy: 0.9712 - val_loss: 0.3407 - val_accuracy: 0.9127\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1078 - accuracy: 0.9734 - val_loss: 0.2906 - val_accuracy: 0.9172\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1162 - accuracy: 0.9714 - val_loss: 0.3036 - val_accuracy: 0.9194\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1184 - accuracy: 0.9681 - val_loss: 0.3457 - val_accuracy: 0.9186\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1081 - accuracy: 0.9749 - val_loss: 0.3679 - val_accuracy: 0.9142\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1022 - accuracy: 0.9734 - val_loss: 0.3213 - val_accuracy: 0.9172\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.0920 - accuracy: 0.9779 - val_loss: 0.3187 - val_accuracy: 0.9172\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.0876 - accuracy: 0.9777 - val_loss: 0.2761 - val_accuracy: 0.9253\n",
      "Fold accuracy: 0.9253\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.6958 - accuracy: 0.7786 - val_loss: 0.5256 - val_accuracy: 0.8351\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.3529 - accuracy: 0.8828 - val_loss: 0.3837 - val_accuracy: 0.8802\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2268 - accuracy: 0.9367 - val_loss: 0.3655 - val_accuracy: 0.8876\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1767 - accuracy: 0.9533 - val_loss: 0.2921 - val_accuracy: 0.9164\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1411 - accuracy: 0.9631 - val_loss: 0.3040 - val_accuracy: 0.9142\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1364 - accuracy: 0.9638 - val_loss: 0.3447 - val_accuracy: 0.9127\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1225 - accuracy: 0.9707 - val_loss: 0.3945 - val_accuracy: 0.9024\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1205 - accuracy: 0.9696 - val_loss: 0.3410 - val_accuracy: 0.9149\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1174 - accuracy: 0.9707 - val_loss: 0.3953 - val_accuracy: 0.8898\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1065 - accuracy: 0.9727 - val_loss: 0.3605 - val_accuracy: 0.8979\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1132 - accuracy: 0.9721 - val_loss: 0.2981 - val_accuracy: 0.9305\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1042 - accuracy: 0.9743 - val_loss: 0.2722 - val_accuracy: 0.9223\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1020 - accuracy: 0.9736 - val_loss: 0.4024 - val_accuracy: 0.9090\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0944 - accuracy: 0.9758 - val_loss: 0.3563 - val_accuracy: 0.9053\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0920 - accuracy: 0.9755 - val_loss: 0.3682 - val_accuracy: 0.9201\n",
      "Fold accuracy: 0.9305\n",
      "Model saved to models/kfold_mosaic\\64px\n",
      "Average accuracy for 64px: 0.9266\n",
      "\n",
      "Cross-validating 100px...\n",
      "Found 6771 images belonging to 5 classes.\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 9s 35ms/step - loss: 0.7484 - accuracy: 0.7608 - val_loss: 0.4822 - val_accuracy: 0.8757\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.3250 - accuracy: 0.8937 - val_loss: 0.3933 - val_accuracy: 0.9038\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2130 - accuracy: 0.9352 - val_loss: 0.3296 - val_accuracy: 0.8883\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1698 - accuracy: 0.9533 - val_loss: 0.3308 - val_accuracy: 0.9120\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1391 - accuracy: 0.9642 - val_loss: 0.3132 - val_accuracy: 0.9186\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1260 - accuracy: 0.9688 - val_loss: 0.3657 - val_accuracy: 0.9127\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1167 - accuracy: 0.9692 - val_loss: 0.3698 - val_accuracy: 0.9075\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1161 - accuracy: 0.9710 - val_loss: 0.3174 - val_accuracy: 0.9216\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1076 - accuracy: 0.9714 - val_loss: 0.3562 - val_accuracy: 0.9186\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1069 - accuracy: 0.9738 - val_loss: 0.3165 - val_accuracy: 0.9194\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1006 - accuracy: 0.9738 - val_loss: 0.2753 - val_accuracy: 0.9305\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 21ms/step - loss: 0.0982 - accuracy: 0.9736 - val_loss: 0.3278 - val_accuracy: 0.9312\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0928 - accuracy: 0.9760 - val_loss: 0.3860 - val_accuracy: 0.9164\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0924 - accuracy: 0.9743 - val_loss: 0.5076 - val_accuracy: 0.8624\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0886 - accuracy: 0.9771 - val_loss: 0.3268 - val_accuracy: 0.9260\n",
      "Fold accuracy: 0.9312\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.7154 - accuracy: 0.7728 - val_loss: 0.4800 - val_accuracy: 0.8772\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.3210 - accuracy: 0.8943 - val_loss: 0.3604 - val_accuracy: 0.8824\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.2040 - accuracy: 0.9433 - val_loss: 0.3394 - val_accuracy: 0.9179\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1550 - accuracy: 0.9611 - val_loss: 0.3166 - val_accuracy: 0.9164\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1321 - accuracy: 0.9673 - val_loss: 0.3520 - val_accuracy: 0.9164\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1247 - accuracy: 0.9664 - val_loss: 0.3261 - val_accuracy: 0.9305\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1196 - accuracy: 0.9697 - val_loss: 0.3278 - val_accuracy: 0.9164\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1085 - accuracy: 0.9740 - val_loss: 0.3967 - val_accuracy: 0.9135\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1046 - accuracy: 0.9736 - val_loss: 0.4076 - val_accuracy: 0.9127\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1028 - accuracy: 0.9760 - val_loss: 0.3167 - val_accuracy: 0.9283\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1035 - accuracy: 0.9753 - val_loss: 0.3676 - val_accuracy: 0.9231\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.0980 - accuracy: 0.9764 - val_loss: 0.3667 - val_accuracy: 0.9268\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1010 - accuracy: 0.9738 - val_loss: 0.3313 - val_accuracy: 0.9260\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0962 - accuracy: 0.9751 - val_loss: 0.4171 - val_accuracy: 0.9194\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0970 - accuracy: 0.9762 - val_loss: 0.3965 - val_accuracy: 0.9238\n",
      "Fold accuracy: 0.9305\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 23ms/step - loss: 0.8095 - accuracy: 0.7363 - val_loss: 0.5356 - val_accuracy: 0.8817\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.3684 - accuracy: 0.8837 - val_loss: 0.3517 - val_accuracy: 0.8839\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.2453 - accuracy: 0.9240 - val_loss: 0.3535 - val_accuracy: 0.8942\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1868 - accuracy: 0.9483 - val_loss: 0.4105 - val_accuracy: 0.8824\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1621 - accuracy: 0.9557 - val_loss: 0.3352 - val_accuracy: 0.9209\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1433 - accuracy: 0.9627 - val_loss: 0.3205 - val_accuracy: 0.9231\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.1399 - accuracy: 0.9607 - val_loss: 0.3075 - val_accuracy: 0.9268\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.1267 - accuracy: 0.9646 - val_loss: 0.3741 - val_accuracy: 0.9105\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1219 - accuracy: 0.9696 - val_loss: 0.3450 - val_accuracy: 0.9172\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1122 - accuracy: 0.9705 - val_loss: 0.4190 - val_accuracy: 0.9157\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1064 - accuracy: 0.9745 - val_loss: 0.3290 - val_accuracy: 0.9283\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.1018 - accuracy: 0.9762 - val_loss: 0.3226 - val_accuracy: 0.9246\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1016 - accuracy: 0.9732 - val_loss: 0.3295 - val_accuracy: 0.9290\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0974 - accuracy: 0.9764 - val_loss: 0.3607 - val_accuracy: 0.9283\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0911 - accuracy: 0.9753 - val_loss: 0.3214 - val_accuracy: 0.9312\n",
      "Fold accuracy: 0.9312\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.7225 - accuracy: 0.7738 - val_loss: 0.4664 - val_accuracy: 0.8824\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.3203 - accuracy: 0.8937 - val_loss: 0.3332 - val_accuracy: 0.8913\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2104 - accuracy: 0.9371 - val_loss: 0.3272 - val_accuracy: 0.9135\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1555 - accuracy: 0.9587 - val_loss: 0.3645 - val_accuracy: 0.9238\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1379 - accuracy: 0.9636 - val_loss: 0.3178 - val_accuracy: 0.9268\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1276 - accuracy: 0.9694 - val_loss: 0.3636 - val_accuracy: 0.9083\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1218 - accuracy: 0.9694 - val_loss: 0.3893 - val_accuracy: 0.9127\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1160 - accuracy: 0.9721 - val_loss: 0.2939 - val_accuracy: 0.9283\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1102 - accuracy: 0.9747 - val_loss: 0.3106 - val_accuracy: 0.9290\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0987 - accuracy: 0.9743 - val_loss: 0.3704 - val_accuracy: 0.9201\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1113 - accuracy: 0.9727 - val_loss: 0.3822 - val_accuracy: 0.9201\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1031 - accuracy: 0.9747 - val_loss: 0.3165 - val_accuracy: 0.9283\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0931 - accuracy: 0.9773 - val_loss: 0.3837 - val_accuracy: 0.9209\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0932 - accuracy: 0.9747 - val_loss: 0.3129 - val_accuracy: 0.9253\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0917 - accuracy: 0.9766 - val_loss: 0.3296 - val_accuracy: 0.9231\n",
      "Fold accuracy: 0.9290\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/15\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.7424 - accuracy: 0.7688 - val_loss: 0.4797 - val_accuracy: 0.8868\n",
      "Epoch 2/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.3346 - accuracy: 0.8904 - val_loss: 0.5034 - val_accuracy: 0.8691\n",
      "Epoch 3/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.2373 - accuracy: 0.9297 - val_loss: 0.3483 - val_accuracy: 0.9090\n",
      "Epoch 4/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1760 - accuracy: 0.9526 - val_loss: 0.3583 - val_accuracy: 0.9238\n",
      "Epoch 5/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1514 - accuracy: 0.9598 - val_loss: 0.3126 - val_accuracy: 0.9216\n",
      "Epoch 6/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1349 - accuracy: 0.9644 - val_loss: 0.3680 - val_accuracy: 0.9105\n",
      "Epoch 7/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1285 - accuracy: 0.9651 - val_loss: 0.3259 - val_accuracy: 0.9172\n",
      "Epoch 8/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.1205 - accuracy: 0.9710 - val_loss: 0.3168 - val_accuracy: 0.9253\n",
      "Epoch 9/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1152 - accuracy: 0.9705 - val_loss: 0.3374 - val_accuracy: 0.9305\n",
      "Epoch 10/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.1044 - accuracy: 0.9742 - val_loss: 0.3235 - val_accuracy: 0.9283\n",
      "Epoch 11/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0981 - accuracy: 0.9758 - val_loss: 0.3363 - val_accuracy: 0.9246\n",
      "Epoch 12/15\n",
      "170/170 [==============================] - 3s 19ms/step - loss: 0.0963 - accuracy: 0.9740 - val_loss: 0.4082 - val_accuracy: 0.9135\n",
      "Epoch 13/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0947 - accuracy: 0.9767 - val_loss: 0.4242 - val_accuracy: 0.9068\n",
      "Epoch 14/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0904 - accuracy: 0.9777 - val_loss: 0.4908 - val_accuracy: 0.9053\n",
      "Epoch 15/15\n",
      "170/170 [==============================] - 3s 20ms/step - loss: 0.0886 - accuracy: 0.9766 - val_loss: 0.3633 - val_accuracy: 0.9283\n",
      "Fold accuracy: 0.9305\n",
      "Model saved to models/kfold_mosaic\\100px\n",
      "Average accuracy for 100px: 0.9305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\2600004036.py:64: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# mosaic version\n",
    "# Iterate through resolution folders (e.g., 4px, 16px)\n",
    "for resolution_folder in sorted(os.listdir(mosaics_dir), key=lambda x: int(re.search(r'\\d+', x).group())):\n",
    "\tresolution_path = os.path.join(mosaics_dir, resolution_folder)\n",
    "\n",
    "\tif os.path.isdir(resolution_path):\n",
    "\t\tprint(f\"\\nCross-validating {resolution_folder}...\")\n",
    "\n",
    "\t\t# Data Preprocessing\n",
    "\t\tdatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\t\tdataset = datagen.flow_from_directory(\n",
    "\t\t\tresolution_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\tclass_mode='categorical', shuffle=True\n",
    "\t\t)\n",
    "\n",
    "\t\tnum_samples = dataset.samples\n",
    "\t\tnum_classes = dataset.num_classes\n",
    "\t\tinput_shape = (img_size[0], img_size[1], 3)\n",
    "\n",
    "\t\t# K-Fold Cross-Validation\n",
    "\t\tkfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\t\tfold_accuracies = []\n",
    "\n",
    "\t\tfor train_idx, val_idx in kfold.split(np.arange(num_samples)):\n",
    "\t\t\t# Re-initialize CNN for each fold\n",
    "\t\t\tmodel = cnn_model(input_shape, num_classes)\n",
    "\n",
    "\t\t\t# Create new data generators for train/validation split\n",
    "\t\t\ttrain_data = datagen.flow_from_directory(\n",
    "\t\t\t\tresolution_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\t\tclass_mode='categorical', subset='training'\n",
    "\t\t\t)\n",
    "\t\t\tval_data = datagen.flow_from_directory(\n",
    "\t\t\t\tresolution_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\t\tclass_mode='categorical', subset='validation'\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Train model\n",
    "\t\t\thistory = model.fit(train_data, validation_data=val_data, epochs=epochs, verbose=1)\n",
    "\n",
    "\t\t\t# Store best validation accuracy for this fold\n",
    "\t\t\tmax_fold_acc = max(history.history['val_accuracy'])\n",
    "\t\t\tfold_accuracies.append(max_fold_acc)\n",
    "\t\t\tprint(f\"Fold accuracy: {max_fold_acc:.4f}\")\n",
    "\n",
    "\t\t# save the model\n",
    "\t\tmodel_path = os.path.join(\"models/kfold_mosaic\", resolution_folder)\n",
    "\t\tos.makedirs(model_path, exist_ok=True)\n",
    "\t\tmodel_name = f\"{resolution_folder}_fold_{len(fold_accuracies)}_mosaic_mask_5.h5\"\n",
    "\t\tmodel.save(os.path.join(model_path, model_name))\n",
    "\t\tprint(f\"Model saved to {model_path}\")\n",
    "\n",
    "\t\t# Store average accuracy for this resolution\n",
    "\t\tavg_acc = np.mean(fold_accuracies)\n",
    "\t\tcross_val_accuracies_mos[resolution_folder] = avg_acc\n",
    "\t\tprint(f\"Average accuracy for {resolution_folder}: {avg_acc:.4f}\")\n",
    "\n",
    "# Generate Bar Graph\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(cross_val_accuracies_mos.keys(), cross_val_accuracies_mos.values(), color='blue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Cross-Validation Avg Accuracy\")\n",
    "plt.title(\"CNN Training Results by Resolution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\1158131985.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Create positions for bars\n",
    "num_bars = len(cross_val_accuracies_mos)\n",
    "x_positions = np.arange(num_bars)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(x_positions, cross_val_accuracies_mos.values(), color='blue', width=0.5)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Custom x-ticks for readability\n",
    "plt.xticks(x_positions, cross_val_accuracies_mos.keys(), rotation=90)\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "\n",
    "plt.ylabel(\"Max Accuracy\")\n",
    "plt.title(\"CNN Validation Results\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/kfold/cnn_validation_accuracy_kfold_mosaics_mask_5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\1598152253.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Extract and sort resolutions numerically\n",
    "resolutions = sorted(cross_val_accuracies_mos.keys(), key=lambda x: int(x.replace(\"px\", \"\")))\n",
    "\n",
    "# Get max accuracy values in the correct order\n",
    "accuracies = [cross_val_accuracies_mos[res] for res in resolutions]\n",
    "\n",
    "# Plot the accuracy per resolution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(resolutions, accuracies, marker=\"o\", linestyle=\"-\", linewidth=2, markersize=8, color=\"blue\", label=\"Accuracy\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Resolution\", fontsize=12)\n",
    "plt.ylabel(\"Max Accuracy\", fontsize=12)\n",
    "plt.title(\"CNN Validation Accuracy by Resolution\", fontsize=14)\n",
    "\n",
    "# Add data points on the plot\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(resolutions[i], acc, f\"{acc:.2f}\", fontsize=10, ha=\"right\")\n",
    "\n",
    "# Grid and legend\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"../../imgs/graphs/kfold/cnn_validation_accuracy_kfold_mosaics_line_mask_5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_274\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_548 (Conv2D)         (None, 16, 46, 32)        2432      \n",
      "                                                                 \n",
      " max_pooling2d_548 (MaxPool  (None, 8, 23, 32)         0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " conv2d_549 (Conv2D)         (None, 4, 19, 64)         51264     \n",
      "                                                                 \n",
      " max_pooling2d_549 (MaxPool  (None, 2, 9, 64)          0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " flatten_274 (Flatten)       (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_548 (Dense)           (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_549 (Dense)           (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201925 (788.77 KB)\n",
      "Trainable params: 201925 (788.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Found 6771 images belonging to 5 classes.\n",
      "212/212 [==============================] - 4s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_23520\\767047091.py:40: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the model for the new version\n",
    "new_model_path = \"models/kfold_mosaic/100px/100px_fold_5_mosaic_mask_5.h5\"\n",
    "new_model = load_model(new_model_path)\n",
    "new_model.summary()\n",
    "\n",
    "# Load the test data for the new version\n",
    "new_test_data_dir = 'C:/Users/theof/OneDrive/Documents/Github/genome_color_unpickler/res/100px'\n",
    "\n",
    "new_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "new_test_generator = new_test_datagen.flow_from_directory(\n",
    "\tnew_test_data_dir,\n",
    "\ttarget_size=(img_size[0], img_size[1]),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='categorical',\n",
    "\tshuffle=False\n",
    ")\n",
    "\n",
    "# Get the true labels for the new version\n",
    "new_true_labels = new_test_generator.classes\n",
    "new_class_labels = list(new_test_generator.class_indices.keys())\n",
    "\n",
    "# Get the predicted labels for the new version\n",
    "new_predictions = new_model.predict(new_test_generator, steps=len(new_test_generator), verbose=1)\n",
    "new_predicted_labels = np.argmax(new_predictions, axis=1)\n",
    "\n",
    "# Generate the confusion matrix for the new version\n",
    "new_cm = confusion_matrix(new_true_labels, new_predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix for the new version\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(new_cm, annot=True, fmt='d', cmap='Blues', xticklabels=new_class_labels, yticklabels=new_class_labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix for New Version')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/kfold/cnn_confusion_matrix_kfold_mosaics_100px_mask_5.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
