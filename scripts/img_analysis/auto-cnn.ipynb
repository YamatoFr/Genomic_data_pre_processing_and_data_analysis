{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "matplotlib.use('Agg')  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "images_dir = \"images\"\n",
    "\n",
    "# Training parameters\n",
    "img_size = (10, 10)  # Resize images to 64x64\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Dictionary to store max accuracies\n",
    "max_accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a simple CNN model\n",
    "def cnn_model(input_shape):\n",
    "\tmodel = Sequential([\n",
    "\t\tConv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "\t\tMaxPooling2D(2,2),\n",
    "\t\tConv2D(64, (3,3), activation='relu'),\n",
    "\t\tMaxPooling2D(2,2),\n",
    "\t\tFlatten(),\n",
    "\t\tDense(128, activation='relu'),\n",
    "\t\tDense(5, activation='softmax')  # Adjust output neurons based on classes\n",
    "\t])\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 4px/Chargaff-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.9872 - accuracy: 0.6510 - val_loss: 0.9577 - val_accuracy: 0.6494\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 14ms/step - loss: 0.9175 - accuracy: 0.6617 - val_loss: 0.9538 - val_accuracy: 0.6442\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 14ms/step - loss: 0.9021 - accuracy: 0.6704 - val_loss: 0.9350 - val_accuracy: 0.6450\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.8796 - accuracy: 0.6815 - val_loss: 0.9300 - val_accuracy: 0.6479\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.8671 - accuracy: 0.6894 - val_loss: 0.9135 - val_accuracy: 0.6464\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.8635 - accuracy: 0.6898 - val_loss: 0.9230 - val_accuracy: 0.6612\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8510 - accuracy: 0.6979 - val_loss: 0.9014 - val_accuracy: 0.6672\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8406 - accuracy: 0.7064 - val_loss: 0.8885 - val_accuracy: 0.6768\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8362 - accuracy: 0.7057 - val_loss: 0.8926 - val_accuracy: 0.6783\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.8382 - accuracy: 0.7044 - val_loss: 0.8796 - val_accuracy: 0.6842\n",
      "Max accuracy for 4px - Chargaff-Composante-Diversite: 0.6842\n",
      "Model saved to models\\4px\\Chargaff-Composante-Diversite\n",
      "\n",
      "Training on 4px/Chargaff-Composante-NucleScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 1.0032 - accuracy: 0.6422 - val_loss: 0.9775 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9795 - accuracy: 0.6510 - val_loss: 0.9677 - val_accuracy: 0.6516\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9707 - accuracy: 0.6510 - val_loss: 0.9752 - val_accuracy: 0.6516\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9555 - accuracy: 0.6534 - val_loss: 0.9371 - val_accuracy: 0.6516\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9381 - accuracy: 0.6634 - val_loss: 0.9046 - val_accuracy: 0.6679\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9188 - accuracy: 0.6784 - val_loss: 0.8807 - val_accuracy: 0.7389\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8890 - accuracy: 0.6942 - val_loss: 0.8332 - val_accuracy: 0.7345\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8721 - accuracy: 0.7057 - val_loss: 0.8509 - val_accuracy: 0.7308\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8504 - accuracy: 0.7237 - val_loss: 0.8511 - val_accuracy: 0.7308\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8357 - accuracy: 0.7302 - val_loss: 0.8072 - val_accuracy: 0.7382\n",
      "Max accuracy for 4px - Chargaff-Composante-NucleScore: 0.7389\n",
      "Model saved to models\\4px\\Chargaff-Composante-NucleScore\n",
      "\n",
      "Training on 4px/Chargaff-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9918 - accuracy: 0.6435 - val_loss: 0.9578 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9224 - accuracy: 0.6584 - val_loss: 0.9384 - val_accuracy: 0.6405\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8961 - accuracy: 0.6719 - val_loss: 0.9394 - val_accuracy: 0.6272\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8843 - accuracy: 0.6758 - val_loss: 0.9283 - val_accuracy: 0.6701\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8800 - accuracy: 0.6782 - val_loss: 0.9359 - val_accuracy: 0.6368\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8597 - accuracy: 0.6926 - val_loss: 0.9156 - val_accuracy: 0.6812\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8385 - accuracy: 0.7129 - val_loss: 0.8703 - val_accuracy: 0.6731\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8160 - accuracy: 0.7247 - val_loss: 0.8978 - val_accuracy: 0.6686\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.7902 - accuracy: 0.7391 - val_loss: 0.8223 - val_accuracy: 0.7056\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7690 - accuracy: 0.7524 - val_loss: 0.7996 - val_accuracy: 0.7271\n",
      "Max accuracy for 4px - Chargaff-Diversite-NucleScore: 0.7271\n",
      "Model saved to models\\4px\\Chargaff-Diversite-NucleScore\n",
      "\n",
      "Training on 4px/Chargaff-Skew-Composante...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 1.0041 - accuracy: 0.6510 - val_loss: 0.9763 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9779 - accuracy: 0.6510 - val_loss: 0.9764 - val_accuracy: 0.6516\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9757 - accuracy: 0.6510 - val_loss: 0.9674 - val_accuracy: 0.6516\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9672 - accuracy: 0.6510 - val_loss: 0.9630 - val_accuracy: 0.6516\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9624 - accuracy: 0.6510 - val_loss: 0.9580 - val_accuracy: 0.6516\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9579 - accuracy: 0.6529 - val_loss: 0.9481 - val_accuracy: 0.6524\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9540 - accuracy: 0.6533 - val_loss: 0.9401 - val_accuracy: 0.6516\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9515 - accuracy: 0.6582 - val_loss: 0.9407 - val_accuracy: 0.6709\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9499 - accuracy: 0.6514 - val_loss: 0.9781 - val_accuracy: 0.6516\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9458 - accuracy: 0.6564 - val_loss: 0.9545 - val_accuracy: 0.6790\n",
      "Max accuracy for 4px - Chargaff-Skew-Composante: 0.6790\n",
      "Model saved to models\\4px\\Chargaff-Skew-Composante\n",
      "\n",
      "Training on 4px/Chargaff-Skew-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.9889 - accuracy: 0.6464 - val_loss: 0.9597 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9175 - accuracy: 0.6641 - val_loss: 0.9454 - val_accuracy: 0.6235\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8991 - accuracy: 0.6677 - val_loss: 0.9370 - val_accuracy: 0.6361\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8836 - accuracy: 0.6737 - val_loss: 0.9390 - val_accuracy: 0.6487\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8768 - accuracy: 0.6824 - val_loss: 0.9250 - val_accuracy: 0.6339\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8727 - accuracy: 0.6820 - val_loss: 0.9197 - val_accuracy: 0.6575\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8629 - accuracy: 0.6889 - val_loss: 0.9401 - val_accuracy: 0.6383\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8650 - accuracy: 0.6885 - val_loss: 0.9432 - val_accuracy: 0.6494\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8579 - accuracy: 0.6861 - val_loss: 0.9663 - val_accuracy: 0.6472\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8543 - accuracy: 0.6920 - val_loss: 0.9177 - val_accuracy: 0.6457\n",
      "Max accuracy for 4px - Chargaff-Skew-Diversite: 0.6575\n",
      "Model saved to models\\4px\\Chargaff-Skew-Diversite\n",
      "\n",
      "Training on 4px/Chargaff-Skew-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 1.0101 - accuracy: 0.6431 - val_loss: 0.9815 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9813 - accuracy: 0.6510 - val_loss: 0.9822 - val_accuracy: 0.6516\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9792 - accuracy: 0.6510 - val_loss: 0.9726 - val_accuracy: 0.6516\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9811 - accuracy: 0.6510 - val_loss: 0.9747 - val_accuracy: 0.6516\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9721 - accuracy: 0.6510 - val_loss: 0.9730 - val_accuracy: 0.6516\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9628 - accuracy: 0.6510 - val_loss: 0.9701 - val_accuracy: 0.6516\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9574 - accuracy: 0.6557 - val_loss: 0.9450 - val_accuracy: 0.6516\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9402 - accuracy: 0.6544 - val_loss: 0.9133 - val_accuracy: 0.6516\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9044 - accuracy: 0.6750 - val_loss: 0.9377 - val_accuracy: 0.6738\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8840 - accuracy: 0.6929 - val_loss: 0.8204 - val_accuracy: 0.7374\n",
      "Max accuracy for 4px - Chargaff-Skew-NucleScore: 0.7374\n",
      "Model saved to models\\4px\\Chargaff-Skew-NucleScore\n",
      "\n",
      "Training on 4px/Composante-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 1.0005 - accuracy: 0.6437 - val_loss: 0.9606 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9191 - accuracy: 0.6632 - val_loss: 0.9762 - val_accuracy: 0.6464\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8970 - accuracy: 0.6785 - val_loss: 0.9308 - val_accuracy: 0.6354\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8828 - accuracy: 0.6822 - val_loss: 0.9215 - val_accuracy: 0.6368\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8733 - accuracy: 0.6854 - val_loss: 0.9141 - val_accuracy: 0.6464\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8703 - accuracy: 0.6833 - val_loss: 0.9498 - val_accuracy: 0.6413\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8568 - accuracy: 0.6961 - val_loss: 0.9015 - val_accuracy: 0.6612\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8455 - accuracy: 0.7025 - val_loss: 0.9151 - val_accuracy: 0.6583\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8420 - accuracy: 0.7051 - val_loss: 0.8905 - val_accuracy: 0.6775\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8341 - accuracy: 0.7106 - val_loss: 0.8748 - val_accuracy: 0.6709\n",
      "Max accuracy for 4px - Composante-Diversite-NucleScore: 0.6775\n",
      "Model saved to models\\4px\\Composante-Diversite-NucleScore\n",
      "\n",
      "Training on 4px/Skew-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.9809 - accuracy: 0.6479 - val_loss: 0.9458 - val_accuracy: 0.6501\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9106 - accuracy: 0.6673 - val_loss: 0.9399 - val_accuracy: 0.6324\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8915 - accuracy: 0.6756 - val_loss: 0.9316 - val_accuracy: 0.6339\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8820 - accuracy: 0.6824 - val_loss: 0.9300 - val_accuracy: 0.6398\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8808 - accuracy: 0.6828 - val_loss: 0.9210 - val_accuracy: 0.6627\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8700 - accuracy: 0.6892 - val_loss: 0.9211 - val_accuracy: 0.6509\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8637 - accuracy: 0.6905 - val_loss: 0.9224 - val_accuracy: 0.6509\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8571 - accuracy: 0.6927 - val_loss: 0.9218 - val_accuracy: 0.6442\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8601 - accuracy: 0.6903 - val_loss: 0.9045 - val_accuracy: 0.6679\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8566 - accuracy: 0.6924 - val_loss: 0.9066 - val_accuracy: 0.6635\n",
      "Max accuracy for 4px - Skew-Composante-Diversite: 0.6679\n",
      "Model saved to models\\4px\\Skew-Composante-Diversite\n",
      "\n",
      "Training on 4px/Skew-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.9958 - accuracy: 0.6510 - val_loss: 0.9948 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9779 - accuracy: 0.6510 - val_loss: 0.9932 - val_accuracy: 0.6516\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9714 - accuracy: 0.6510 - val_loss: 0.9577 - val_accuracy: 0.6516\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9584 - accuracy: 0.6510 - val_loss: 0.9448 - val_accuracy: 0.6516\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9482 - accuracy: 0.6542 - val_loss: 0.9764 - val_accuracy: 0.6975\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9483 - accuracy: 0.6569 - val_loss: 0.9379 - val_accuracy: 0.6524\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9351 - accuracy: 0.6616 - val_loss: 0.9230 - val_accuracy: 0.6524\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.9318 - accuracy: 0.6621 - val_loss: 0.9192 - val_accuracy: 0.6516\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9194 - accuracy: 0.6630 - val_loss: 0.9018 - val_accuracy: 0.6501\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9102 - accuracy: 0.6743 - val_loss: 0.9160 - val_accuracy: 0.6834\n",
      "Max accuracy for 4px - Skew-Composante-NucleScore: 0.6975\n",
      "Model saved to models\\4px\\Skew-Composante-NucleScore\n",
      "\n",
      "Training on 4px/Skew-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.9955 - accuracy: 0.6474 - val_loss: 0.9623 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9210 - accuracy: 0.6605 - val_loss: 0.9517 - val_accuracy: 0.6391\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8950 - accuracy: 0.6724 - val_loss: 0.9514 - val_accuracy: 0.6472\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8807 - accuracy: 0.6789 - val_loss: 0.9717 - val_accuracy: 0.6531\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8752 - accuracy: 0.6881 - val_loss: 0.9214 - val_accuracy: 0.6472\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.8619 - accuracy: 0.6935 - val_loss: 0.9090 - val_accuracy: 0.6561\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8585 - accuracy: 0.6937 - val_loss: 0.9156 - val_accuracy: 0.6509\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8448 - accuracy: 0.6988 - val_loss: 0.9178 - val_accuracy: 0.6538\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8439 - accuracy: 0.7031 - val_loss: 0.8775 - val_accuracy: 0.6857\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8367 - accuracy: 0.7014 - val_loss: 0.8904 - val_accuracy: 0.6538\n",
      "Max accuracy for 4px - Skew-Diversite-NucleScore: 0.6857\n",
      "Model saved to models\\4px\\Skew-Diversite-NucleScore\n",
      "\n",
      "Training on 16px/Chargaff-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.9834 - accuracy: 0.6507 - val_loss: 0.9253 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8525 - accuracy: 0.6985 - val_loss: 0.7625 - val_accuracy: 0.7552\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6672 - accuracy: 0.7898 - val_loss: 0.6282 - val_accuracy: 0.7818\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5779 - accuracy: 0.8173 - val_loss: 0.5325 - val_accuracy: 0.8240\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5180 - accuracy: 0.8323 - val_loss: 0.4750 - val_accuracy: 0.8410\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4774 - accuracy: 0.8465 - val_loss: 0.4567 - val_accuracy: 0.8462\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4397 - accuracy: 0.8620 - val_loss: 0.4201 - val_accuracy: 0.8831\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4297 - accuracy: 0.8618 - val_loss: 0.4197 - val_accuracy: 0.8787\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4075 - accuracy: 0.8677 - val_loss: 0.4237 - val_accuracy: 0.8410\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3967 - accuracy: 0.8688 - val_loss: 0.3968 - val_accuracy: 0.8720\n",
      "Max accuracy for 16px - Chargaff-Composante-Diversite: 0.8831\n",
      "Model saved to models\\16px\\Chargaff-Composante-Diversite\n",
      "\n",
      "Training on 16px/Chargaff-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 1.0085 - accuracy: 0.6446 - val_loss: 0.9831 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9269 - accuracy: 0.6586 - val_loss: 0.8118 - val_accuracy: 0.6672\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6815 - accuracy: 0.8129 - val_loss: 0.5843 - val_accuracy: 0.8499\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6022 - accuracy: 0.8352 - val_loss: 0.5935 - val_accuracy: 0.8247\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5765 - accuracy: 0.8387 - val_loss: 0.5407 - val_accuracy: 0.8380\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5491 - accuracy: 0.8404 - val_loss: 0.5037 - val_accuracy: 0.8602\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5276 - accuracy: 0.8437 - val_loss: 0.5322 - val_accuracy: 0.8314\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5002 - accuracy: 0.8496 - val_loss: 0.4491 - val_accuracy: 0.8691\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4705 - accuracy: 0.8590 - val_loss: 0.4170 - val_accuracy: 0.8757\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4579 - accuracy: 0.8671 - val_loss: 0.4154 - val_accuracy: 0.8780\n",
      "Max accuracy for 16px - Chargaff-Composante-NucleScore: 0.8780\n",
      "Model saved to models\\16px\\Chargaff-Composante-NucleScore\n",
      "\n",
      "Training on 16px/Chargaff-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.9887 - accuracy: 0.6433 - val_loss: 0.9499 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8929 - accuracy: 0.6785 - val_loss: 0.8871 - val_accuracy: 0.6531\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7056 - accuracy: 0.7749 - val_loss: 0.6199 - val_accuracy: 0.7959\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5600 - accuracy: 0.8225 - val_loss: 0.5159 - val_accuracy: 0.8447\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4812 - accuracy: 0.8507 - val_loss: 0.4618 - val_accuracy: 0.8410\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4407 - accuracy: 0.8596 - val_loss: 0.4109 - val_accuracy: 0.8698\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4049 - accuracy: 0.8749 - val_loss: 0.3996 - val_accuracy: 0.8691\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3822 - accuracy: 0.8797 - val_loss: 0.4356 - val_accuracy: 0.8343\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3630 - accuracy: 0.8893 - val_loss: 0.4348 - val_accuracy: 0.8351\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3537 - accuracy: 0.8911 - val_loss: 0.3664 - val_accuracy: 0.8669\n",
      "Max accuracy for 16px - Chargaff-Diversite-NucleScore: 0.8698\n",
      "Model saved to models\\16px\\Chargaff-Diversite-NucleScore\n",
      "\n",
      "Training on 16px/Chargaff-Skew-Composante...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 19ms/step - loss: 0.9303 - accuracy: 0.6704 - val_loss: 0.9515 - val_accuracy: 0.7078\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7919 - accuracy: 0.7476 - val_loss: 0.9110 - val_accuracy: 0.7027\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6986 - accuracy: 0.7872 - val_loss: 0.7706 - val_accuracy: 0.7470\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6298 - accuracy: 0.8136 - val_loss: 0.6540 - val_accuracy: 0.7870\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5970 - accuracy: 0.8219 - val_loss: 0.7396 - val_accuracy: 0.7596\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5583 - accuracy: 0.8347 - val_loss: 0.6269 - val_accuracy: 0.7907\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5255 - accuracy: 0.8378 - val_loss: 0.6335 - val_accuracy: 0.7922\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5092 - accuracy: 0.8448 - val_loss: 0.5081 - val_accuracy: 0.8521\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.5053 - accuracy: 0.8424 - val_loss: 0.4943 - val_accuracy: 0.8469\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.4841 - accuracy: 0.8490 - val_loss: 0.5335 - val_accuracy: 0.8166\n",
      "Max accuracy for 16px - Chargaff-Skew-Composante: 0.8521\n",
      "Model saved to models\\16px\\Chargaff-Skew-Composante\n",
      "\n",
      "Training on 16px/Chargaff-Skew-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.8901 - accuracy: 0.6963 - val_loss: 0.9782 - val_accuracy: 0.6546\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7575 - accuracy: 0.7525 - val_loss: 0.7982 - val_accuracy: 0.7115\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6580 - accuracy: 0.7824 - val_loss: 0.7238 - val_accuracy: 0.7448\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5640 - accuracy: 0.8101 - val_loss: 0.6399 - val_accuracy: 0.7751\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5028 - accuracy: 0.8315 - val_loss: 0.6419 - val_accuracy: 0.7781\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4602 - accuracy: 0.8470 - val_loss: 0.5598 - val_accuracy: 0.7959\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.4335 - accuracy: 0.8570 - val_loss: 0.5537 - val_accuracy: 0.7936\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4246 - accuracy: 0.8588 - val_loss: 0.5217 - val_accuracy: 0.8114\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3974 - accuracy: 0.8695 - val_loss: 0.5590 - val_accuracy: 0.7959\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3734 - accuracy: 0.8773 - val_loss: 0.4829 - val_accuracy: 0.8217\n",
      "Max accuracy for 16px - Chargaff-Skew-Diversite: 0.8217\n",
      "Model saved to models\\16px\\Chargaff-Skew-Diversite\n",
      "\n",
      "Training on 16px/Chargaff-Skew-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.9474 - accuracy: 0.6658 - val_loss: 0.9293 - val_accuracy: 0.6746\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8321 - accuracy: 0.7166 - val_loss: 0.8789 - val_accuracy: 0.7019\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7576 - accuracy: 0.7638 - val_loss: 0.8966 - val_accuracy: 0.7256\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.6938 - accuracy: 0.7863 - val_loss: 0.7244 - val_accuracy: 0.7522\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6657 - accuracy: 0.7972 - val_loss: 0.6986 - val_accuracy: 0.7633\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6200 - accuracy: 0.8116 - val_loss: 0.7517 - val_accuracy: 0.7567\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5780 - accuracy: 0.8256 - val_loss: 0.6248 - val_accuracy: 0.7803\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5378 - accuracy: 0.8343 - val_loss: 0.5505 - val_accuracy: 0.8247\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5126 - accuracy: 0.8419 - val_loss: 0.5195 - val_accuracy: 0.8506\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4981 - accuracy: 0.8435 - val_loss: 0.6013 - val_accuracy: 0.7877\n",
      "Max accuracy for 16px - Chargaff-Skew-NucleScore: 0.8506\n",
      "Model saved to models\\16px\\Chargaff-Skew-NucleScore\n",
      "\n",
      "Training on 16px/Composante-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.9821 - accuracy: 0.6472 - val_loss: 0.9816 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8857 - accuracy: 0.6828 - val_loss: 0.8155 - val_accuracy: 0.6834\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7059 - accuracy: 0.7732 - val_loss: 0.6488 - val_accuracy: 0.7988\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5869 - accuracy: 0.8134 - val_loss: 0.6012 - val_accuracy: 0.8321\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5097 - accuracy: 0.8335 - val_loss: 0.6571 - val_accuracy: 0.7522\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4687 - accuracy: 0.8479 - val_loss: 0.5254 - val_accuracy: 0.8351\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4183 - accuracy: 0.8644 - val_loss: 0.5169 - val_accuracy: 0.8084\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4044 - accuracy: 0.8671 - val_loss: 0.4637 - val_accuracy: 0.8365\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3773 - accuracy: 0.8775 - val_loss: 0.5309 - val_accuracy: 0.8070\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3607 - accuracy: 0.8847 - val_loss: 0.4208 - val_accuracy: 0.8543\n",
      "Max accuracy for 16px - Composante-Diversite-NucleScore: 0.8543\n",
      "Model saved to models\\16px\\Composante-Diversite-NucleScore\n",
      "\n",
      "Training on 16px/Skew-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 20ms/step - loss: 0.9161 - accuracy: 0.6774 - val_loss: 0.8873 - val_accuracy: 0.6775\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7739 - accuracy: 0.7354 - val_loss: 0.8678 - val_accuracy: 0.6657\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6575 - accuracy: 0.7859 - val_loss: 0.7000 - val_accuracy: 0.7419\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5737 - accuracy: 0.8134 - val_loss: 0.6222 - val_accuracy: 0.7929\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5031 - accuracy: 0.8372 - val_loss: 0.5697 - val_accuracy: 0.8143\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.4675 - accuracy: 0.8455 - val_loss: 0.5832 - val_accuracy: 0.7981\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4408 - accuracy: 0.8583 - val_loss: 0.6298 - val_accuracy: 0.7811\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4264 - accuracy: 0.8586 - val_loss: 0.5025 - val_accuracy: 0.8343\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4051 - accuracy: 0.8729 - val_loss: 0.5384 - val_accuracy: 0.8040\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3889 - accuracy: 0.8767 - val_loss: 0.5409 - val_accuracy: 0.8010\n",
      "Max accuracy for 16px - Skew-Composante-Diversite: 0.8343\n",
      "Model saved to models\\16px\\Skew-Composante-Diversite\n",
      "\n",
      "Training on 16px/Skew-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.9120 - accuracy: 0.6889 - val_loss: 0.9061 - val_accuracy: 0.6760\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8067 - accuracy: 0.7354 - val_loss: 0.8929 - val_accuracy: 0.7204\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7039 - accuracy: 0.7832 - val_loss: 0.7222 - val_accuracy: 0.7567\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6332 - accuracy: 0.8142 - val_loss: 0.6334 - val_accuracy: 0.8136\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5797 - accuracy: 0.8308 - val_loss: 0.6811 - val_accuracy: 0.7641\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5538 - accuracy: 0.8328 - val_loss: 0.6241 - val_accuracy: 0.7981\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5208 - accuracy: 0.8426 - val_loss: 0.5372 - val_accuracy: 0.8365\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5038 - accuracy: 0.8448 - val_loss: 0.5140 - val_accuracy: 0.8447\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4856 - accuracy: 0.8520 - val_loss: 0.5683 - val_accuracy: 0.8217\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4623 - accuracy: 0.8609 - val_loss: 0.4770 - val_accuracy: 0.8713\n",
      "Max accuracy for 16px - Skew-Composante-NucleScore: 0.8713\n",
      "Model saved to models\\16px\\Skew-Composante-NucleScore\n",
      "\n",
      "Training on 16px/Skew-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9103 - accuracy: 0.6832 - val_loss: 0.8855 - val_accuracy: 0.6805\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7742 - accuracy: 0.7394 - val_loss: 0.8506 - val_accuracy: 0.7115\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7006 - accuracy: 0.7680 - val_loss: 0.7672 - val_accuracy: 0.7078\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6361 - accuracy: 0.7907 - val_loss: 0.7067 - val_accuracy: 0.7478\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.5829 - accuracy: 0.8035 - val_loss: 0.6908 - val_accuracy: 0.7626\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5188 - accuracy: 0.8267 - val_loss: 0.7509 - val_accuracy: 0.7522\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4757 - accuracy: 0.8443 - val_loss: 0.5891 - val_accuracy: 0.7959\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 11ms/step - loss: 0.4519 - accuracy: 0.8535 - val_loss: 0.5603 - val_accuracy: 0.8010\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4237 - accuracy: 0.8651 - val_loss: 0.5613 - val_accuracy: 0.7988\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4070 - accuracy: 0.8699 - val_loss: 0.5376 - val_accuracy: 0.8092\n",
      "Max accuracy for 16px - Skew-Diversite-NucleScore: 0.8092\n",
      "Model saved to models\\16px\\Skew-Diversite-NucleScore\n",
      "\n",
      "Training on 36px/Chargaff-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 19ms/step - loss: 1.0039 - accuracy: 0.6446 - val_loss: 0.9711 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.8307 - accuracy: 0.7166 - val_loss: 0.6578 - val_accuracy: 0.7744\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5005 - accuracy: 0.8601 - val_loss: 0.3992 - val_accuracy: 0.8876\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.3700 - accuracy: 0.8789 - val_loss: 0.3640 - val_accuracy: 0.8735\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2941 - accuracy: 0.9031 - val_loss: 0.3048 - val_accuracy: 0.8883\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2411 - accuracy: 0.9288 - val_loss: 0.3114 - val_accuracy: 0.9016\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2077 - accuracy: 0.9384 - val_loss: 0.3203 - val_accuracy: 0.9038\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1976 - accuracy: 0.9446 - val_loss: 0.2953 - val_accuracy: 0.9024\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1812 - accuracy: 0.9496 - val_loss: 0.3726 - val_accuracy: 0.9068\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1669 - accuracy: 0.9548 - val_loss: 0.3397 - val_accuracy: 0.9149\n",
      "Max accuracy for 36px - Chargaff-Composante-Diversite: 0.9149\n",
      "Model saved to models\\36px\\Chargaff-Composante-Diversite\n",
      "\n",
      "Training on 36px/Chargaff-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.9927 - accuracy: 0.6481 - val_loss: 0.9357 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6933 - accuracy: 0.7887 - val_loss: 0.4667 - val_accuracy: 0.8891\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4242 - accuracy: 0.8740 - val_loss: 0.3609 - val_accuracy: 0.8898\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3363 - accuracy: 0.8987 - val_loss: 0.3301 - val_accuracy: 0.8950\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2788 - accuracy: 0.9197 - val_loss: 0.3272 - val_accuracy: 0.8905\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2494 - accuracy: 0.9299 - val_loss: 0.3302 - val_accuracy: 0.9046\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2169 - accuracy: 0.9376 - val_loss: 0.3060 - val_accuracy: 0.9068\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2066 - accuracy: 0.9437 - val_loss: 0.3074 - val_accuracy: 0.8972\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1977 - accuracy: 0.9469 - val_loss: 0.2952 - val_accuracy: 0.9172\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1856 - accuracy: 0.9517 - val_loss: 0.3501 - val_accuracy: 0.8868\n",
      "Max accuracy for 36px - Chargaff-Composante-NucleScore: 0.9172\n",
      "Model saved to models\\36px\\Chargaff-Composante-NucleScore\n",
      "\n",
      "Training on 36px/Chargaff-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 19ms/step - loss: 1.0008 - accuracy: 0.6438 - val_loss: 0.9382 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.8175 - accuracy: 0.7193 - val_loss: 0.6210 - val_accuracy: 0.8084\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5185 - accuracy: 0.8526 - val_loss: 0.4288 - val_accuracy: 0.8706\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4099 - accuracy: 0.8706 - val_loss: 0.3456 - val_accuracy: 0.8883\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.3336 - accuracy: 0.8932 - val_loss: 0.3074 - val_accuracy: 0.8964\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2724 - accuracy: 0.9138 - val_loss: 0.2932 - val_accuracy: 0.8913\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2407 - accuracy: 0.9230 - val_loss: 0.3000 - val_accuracy: 0.8950\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2177 - accuracy: 0.9312 - val_loss: 0.3528 - val_accuracy: 0.9046\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.2036 - accuracy: 0.9387 - val_loss: 0.3265 - val_accuracy: 0.9090\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1899 - accuracy: 0.9448 - val_loss: 0.3150 - val_accuracy: 0.9194\n",
      "Max accuracy for 36px - Chargaff-Diversite-NucleScore: 0.9194\n",
      "Model saved to models\\36px\\Chargaff-Diversite-NucleScore\n",
      "\n",
      "Training on 36px/Chargaff-Skew-Composante...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9824 - accuracy: 0.6442 - val_loss: 0.8839 - val_accuracy: 0.6524\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6557 - accuracy: 0.7946 - val_loss: 0.4945 - val_accuracy: 0.8780\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4103 - accuracy: 0.8734 - val_loss: 0.3482 - val_accuracy: 0.8787\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3100 - accuracy: 0.9046 - val_loss: 0.3011 - val_accuracy: 0.8883\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2609 - accuracy: 0.9269 - val_loss: 0.3088 - val_accuracy: 0.8964\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.2332 - accuracy: 0.9317 - val_loss: 0.2738 - val_accuracy: 0.9083\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2175 - accuracy: 0.9371 - val_loss: 0.2762 - val_accuracy: 0.9120\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1977 - accuracy: 0.9459 - val_loss: 0.3031 - val_accuracy: 0.9061\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1894 - accuracy: 0.9459 - val_loss: 0.3190 - val_accuracy: 0.9164\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1810 - accuracy: 0.9509 - val_loss: 0.2754 - val_accuracy: 0.9201\n",
      "Max accuracy for 36px - Chargaff-Skew-Composante: 0.9201\n",
      "Model saved to models\\36px\\Chargaff-Skew-Composante\n",
      "\n",
      "Training on 36px/Chargaff-Skew-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9951 - accuracy: 0.6431 - val_loss: 0.9217 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7437 - accuracy: 0.7481 - val_loss: 0.6229 - val_accuracy: 0.8151\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4950 - accuracy: 0.8413 - val_loss: 0.4095 - val_accuracy: 0.8802\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3657 - accuracy: 0.8865 - val_loss: 0.3589 - val_accuracy: 0.8839\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3008 - accuracy: 0.9059 - val_loss: 0.3313 - val_accuracy: 0.8898\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2658 - accuracy: 0.9238 - val_loss: 0.3415 - val_accuracy: 0.8876\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2316 - accuracy: 0.9317 - val_loss: 0.3343 - val_accuracy: 0.8950\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2113 - accuracy: 0.9391 - val_loss: 0.3307 - val_accuracy: 0.8994\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1976 - accuracy: 0.9463 - val_loss: 0.3585 - val_accuracy: 0.8987\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1907 - accuracy: 0.9443 - val_loss: 0.3109 - val_accuracy: 0.9031\n",
      "Max accuracy for 36px - Chargaff-Skew-Diversite: 0.9031\n",
      "Model saved to models\\36px\\Chargaff-Skew-Diversite\n",
      "\n",
      "Training on 36px/Chargaff-Skew-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9757 - accuracy: 0.6510 - val_loss: 0.9153 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7250 - accuracy: 0.7636 - val_loss: 0.5581 - val_accuracy: 0.8632\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4546 - accuracy: 0.8655 - val_loss: 0.3899 - val_accuracy: 0.8854\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3645 - accuracy: 0.8854 - val_loss: 0.3478 - val_accuracy: 0.8913\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3038 - accuracy: 0.9088 - val_loss: 0.2985 - val_accuracy: 0.8994\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2571 - accuracy: 0.9243 - val_loss: 0.2749 - val_accuracy: 0.9009\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2189 - accuracy: 0.9365 - val_loss: 0.3566 - val_accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2078 - accuracy: 0.9391 - val_loss: 0.3259 - val_accuracy: 0.8994\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1929 - accuracy: 0.9457 - val_loss: 0.2886 - val_accuracy: 0.9194\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1776 - accuracy: 0.9483 - val_loss: 0.3185 - val_accuracy: 0.9142\n",
      "Max accuracy for 36px - Chargaff-Skew-NucleScore: 0.9194\n",
      "Model saved to models\\36px\\Chargaff-Skew-NucleScore\n",
      "\n",
      "Training on 36px/Composante-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 1.0023 - accuracy: 0.6461 - val_loss: 0.9375 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7355 - accuracy: 0.7664 - val_loss: 0.5034 - val_accuracy: 0.8817\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4450 - accuracy: 0.8703 - val_loss: 0.3710 - val_accuracy: 0.8898\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3368 - accuracy: 0.8926 - val_loss: 0.3397 - val_accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2605 - accuracy: 0.9201 - val_loss: 0.3244 - val_accuracy: 0.8883\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2261 - accuracy: 0.9326 - val_loss: 0.3040 - val_accuracy: 0.9024\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2053 - accuracy: 0.9404 - val_loss: 0.2797 - val_accuracy: 0.9061\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1812 - accuracy: 0.9500 - val_loss: 0.3177 - val_accuracy: 0.9068\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1772 - accuracy: 0.9529 - val_loss: 0.3367 - val_accuracy: 0.9098\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1653 - accuracy: 0.9539 - val_loss: 0.3315 - val_accuracy: 0.9194\n",
      "Max accuracy for 36px - Composante-Diversite-NucleScore: 0.9194\n",
      "Model saved to models\\36px\\Composante-Diversite-NucleScore\n",
      "\n",
      "Training on 36px/Skew-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.9774 - accuracy: 0.6483 - val_loss: 0.8799 - val_accuracy: 0.6923\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6831 - accuracy: 0.7815 - val_loss: 0.5621 - val_accuracy: 0.7907\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4589 - accuracy: 0.8603 - val_loss: 0.4196 - val_accuracy: 0.8720\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3608 - accuracy: 0.8887 - val_loss: 0.3405 - val_accuracy: 0.8913\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2959 - accuracy: 0.9101 - val_loss: 0.3682 - val_accuracy: 0.8950\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2455 - accuracy: 0.9267 - val_loss: 0.3168 - val_accuracy: 0.8964\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2223 - accuracy: 0.9362 - val_loss: 0.3111 - val_accuracy: 0.8987\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2070 - accuracy: 0.9411 - val_loss: 0.3591 - val_accuracy: 0.8757\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1940 - accuracy: 0.9426 - val_loss: 0.3014 - val_accuracy: 0.9083\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1822 - accuracy: 0.9504 - val_loss: 0.3090 - val_accuracy: 0.9216\n",
      "Max accuracy for 36px - Skew-Composante-Diversite: 0.9216\n",
      "Model saved to models\\36px\\Skew-Composante-Diversite\n",
      "\n",
      "Training on 36px/Skew-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9543 - accuracy: 0.6610 - val_loss: 0.8565 - val_accuracy: 0.7293\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5895 - accuracy: 0.8304 - val_loss: 0.4845 - val_accuracy: 0.8743\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4019 - accuracy: 0.8760 - val_loss: 0.3648 - val_accuracy: 0.8817\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3099 - accuracy: 0.9044 - val_loss: 0.3303 - val_accuracy: 0.8905\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2579 - accuracy: 0.9245 - val_loss: 0.3052 - val_accuracy: 0.9009\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2261 - accuracy: 0.9356 - val_loss: 0.3088 - val_accuracy: 0.9038\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2073 - accuracy: 0.9426 - val_loss: 0.3135 - val_accuracy: 0.9149\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1940 - accuracy: 0.9467 - val_loss: 0.3408 - val_accuracy: 0.9179\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1841 - accuracy: 0.9469 - val_loss: 0.2851 - val_accuracy: 0.9157\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1721 - accuracy: 0.9529 - val_loss: 0.3240 - val_accuracy: 0.9283\n",
      "Max accuracy for 36px - Skew-Composante-NucleScore: 0.9283\n",
      "Model saved to models\\36px\\Skew-Composante-NucleScore\n",
      "\n",
      "Training on 36px/Skew-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9624 - accuracy: 0.6569 - val_loss: 0.8954 - val_accuracy: 0.6524\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7142 - accuracy: 0.7607 - val_loss: 0.6235 - val_accuracy: 0.7544\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5178 - accuracy: 0.8369 - val_loss: 0.4232 - val_accuracy: 0.8735\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3871 - accuracy: 0.8789 - val_loss: 0.3638 - val_accuracy: 0.8817\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3174 - accuracy: 0.8996 - val_loss: 0.3557 - val_accuracy: 0.8891\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2650 - accuracy: 0.9171 - val_loss: 0.3840 - val_accuracy: 0.8942\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2441 - accuracy: 0.9266 - val_loss: 0.3960 - val_accuracy: 0.9024\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2308 - accuracy: 0.9291 - val_loss: 0.3363 - val_accuracy: 0.8876\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2101 - accuracy: 0.9376 - val_loss: 0.3835 - val_accuracy: 0.8898\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2048 - accuracy: 0.9409 - val_loss: 0.3927 - val_accuracy: 0.9038\n",
      "Max accuracy for 36px - Skew-Diversite-NucleScore: 0.9038\n",
      "Model saved to models\\36px\\Skew-Diversite-NucleScore\n",
      "\n",
      "Training on 64px/Chargaff-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 1.0018 - accuracy: 0.6507 - val_loss: 0.9712 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.8412 - accuracy: 0.7071 - val_loss: 0.6546 - val_accuracy: 0.8646\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4636 - accuracy: 0.8693 - val_loss: 0.4543 - val_accuracy: 0.8698\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3588 - accuracy: 0.8795 - val_loss: 0.4031 - val_accuracy: 0.8728\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2999 - accuracy: 0.8950 - val_loss: 0.3583 - val_accuracy: 0.8928\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2611 - accuracy: 0.9183 - val_loss: 0.3458 - val_accuracy: 0.8972\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2377 - accuracy: 0.9262 - val_loss: 0.3336 - val_accuracy: 0.9090\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2111 - accuracy: 0.9415 - val_loss: 0.3437 - val_accuracy: 0.8972\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1916 - accuracy: 0.9493 - val_loss: 0.3441 - val_accuracy: 0.9046\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1807 - accuracy: 0.9504 - val_loss: 0.3593 - val_accuracy: 0.9112\n",
      "Max accuracy for 64px - Chargaff-Composante-Diversite: 0.9112\n",
      "Model saved to models\\64px\\Chargaff-Composante-Diversite\n",
      "\n",
      "Training on 64px/Chargaff-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 19ms/step - loss: 0.9975 - accuracy: 0.6509 - val_loss: 0.9753 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9395 - accuracy: 0.6562 - val_loss: 0.8685 - val_accuracy: 0.6524\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6166 - accuracy: 0.8254 - val_loss: 0.5084 - val_accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4068 - accuracy: 0.8710 - val_loss: 0.4368 - val_accuracy: 0.8720\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3256 - accuracy: 0.8948 - val_loss: 0.4025 - val_accuracy: 0.8772\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2886 - accuracy: 0.9083 - val_loss: 0.3929 - val_accuracy: 0.8979\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2718 - accuracy: 0.9155 - val_loss: 0.4027 - val_accuracy: 0.8839\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2495 - accuracy: 0.9249 - val_loss: 0.3448 - val_accuracy: 0.9075\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2243 - accuracy: 0.9332 - val_loss: 0.3374 - val_accuracy: 0.9194\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2128 - accuracy: 0.9397 - val_loss: 0.3117 - val_accuracy: 0.9186\n",
      "Max accuracy for 64px - Chargaff-Composante-NucleScore: 0.9194\n",
      "Model saved to models\\64px\\Chargaff-Composante-NucleScore\n",
      "\n",
      "Training on 64px/Chargaff-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.9951 - accuracy: 0.6510 - val_loss: 0.9678 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.9019 - accuracy: 0.6673 - val_loss: 0.8294 - val_accuracy: 0.6561\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5530 - accuracy: 0.8498 - val_loss: 0.4659 - val_accuracy: 0.8728\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3835 - accuracy: 0.8786 - val_loss: 0.3902 - val_accuracy: 0.8743\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3252 - accuracy: 0.8872 - val_loss: 0.3665 - val_accuracy: 0.8765\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2848 - accuracy: 0.9066 - val_loss: 0.4260 - val_accuracy: 0.8809\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2507 - accuracy: 0.9206 - val_loss: 0.3567 - val_accuracy: 0.8913\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2357 - accuracy: 0.9295 - val_loss: 0.3212 - val_accuracy: 0.9009\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2103 - accuracy: 0.9411 - val_loss: 0.3018 - val_accuracy: 0.9046\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1955 - accuracy: 0.9452 - val_loss: 0.3079 - val_accuracy: 0.9053\n",
      "Max accuracy for 64px - Chargaff-Diversite-NucleScore: 0.9053\n",
      "Model saved to models\\64px\\Chargaff-Diversite-NucleScore\n",
      "\n",
      "Training on 64px/Chargaff-Skew-Composante...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 19ms/step - loss: 0.8938 - accuracy: 0.7064 - val_loss: 0.8089 - val_accuracy: 0.7330\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5779 - accuracy: 0.8269 - val_loss: 0.6196 - val_accuracy: 0.7996\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4586 - accuracy: 0.8516 - val_loss: 0.6130 - val_accuracy: 0.8077\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.3587 - accuracy: 0.8756 - val_loss: 0.4582 - val_accuracy: 0.8772\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2937 - accuracy: 0.9066 - val_loss: 0.4089 - val_accuracy: 0.8728\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2487 - accuracy: 0.9256 - val_loss: 0.3528 - val_accuracy: 0.8942\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2204 - accuracy: 0.9373 - val_loss: 0.3632 - val_accuracy: 0.8964\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1938 - accuracy: 0.9496 - val_loss: 0.3225 - val_accuracy: 0.9090\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1807 - accuracy: 0.9509 - val_loss: 0.2996 - val_accuracy: 0.9098\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1674 - accuracy: 0.9563 - val_loss: 0.3123 - val_accuracy: 0.9098\n",
      "Max accuracy for 64px - Chargaff-Skew-Composante: 0.9098\n",
      "Model saved to models\\64px\\Chargaff-Skew-Composante\n",
      "\n",
      "Training on 64px/Chargaff-Skew-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 5s 19ms/step - loss: 0.8757 - accuracy: 0.7081 - val_loss: 0.8190 - val_accuracy: 0.7175\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5949 - accuracy: 0.8164 - val_loss: 0.6445 - val_accuracy: 0.8010\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4850 - accuracy: 0.8496 - val_loss: 0.5870 - val_accuracy: 0.8151\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4120 - accuracy: 0.8697 - val_loss: 0.4972 - val_accuracy: 0.8469\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3538 - accuracy: 0.8856 - val_loss: 0.4741 - val_accuracy: 0.8499\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2991 - accuracy: 0.9044 - val_loss: 0.4092 - val_accuracy: 0.8891\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2558 - accuracy: 0.9225 - val_loss: 0.4086 - val_accuracy: 0.8772\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2405 - accuracy: 0.9302 - val_loss: 0.3644 - val_accuracy: 0.8935\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2022 - accuracy: 0.9469 - val_loss: 0.3490 - val_accuracy: 0.9001\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1812 - accuracy: 0.9517 - val_loss: 0.3351 - val_accuracy: 0.9038\n",
      "Max accuracy for 64px - Chargaff-Skew-Diversite: 0.9038\n",
      "Model saved to models\\64px\\Chargaff-Skew-Diversite\n",
      "\n",
      "Training on 64px/Chargaff-Skew-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 18ms/step - loss: 0.9342 - accuracy: 0.6811 - val_loss: 0.9534 - val_accuracy: 0.7078\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6864 - accuracy: 0.7930 - val_loss: 0.7063 - val_accuracy: 0.7389\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5264 - accuracy: 0.8337 - val_loss: 0.6164 - val_accuracy: 0.7929\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4476 - accuracy: 0.8557 - val_loss: 0.5784 - val_accuracy: 0.8195\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3705 - accuracy: 0.8764 - val_loss: 0.4975 - val_accuracy: 0.8462\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3159 - accuracy: 0.9018 - val_loss: 0.4265 - val_accuracy: 0.8646\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2701 - accuracy: 0.9179 - val_loss: 0.3657 - val_accuracy: 0.8987\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2299 - accuracy: 0.9350 - val_loss: 0.3425 - val_accuracy: 0.9009\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2061 - accuracy: 0.9448 - val_loss: 0.3258 - val_accuracy: 0.9046\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1925 - accuracy: 0.9500 - val_loss: 0.3033 - val_accuracy: 0.9083\n",
      "Max accuracy for 64px - Chargaff-Skew-NucleScore: 0.9083\n",
      "Model saved to models\\64px\\Chargaff-Skew-NucleScore\n",
      "\n",
      "Training on 64px/Composante-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9794 - accuracy: 0.6488 - val_loss: 0.9554 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6528 - accuracy: 0.7872 - val_loss: 0.4515 - val_accuracy: 0.8706\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3680 - accuracy: 0.8801 - val_loss: 0.3923 - val_accuracy: 0.8794\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3044 - accuracy: 0.8930 - val_loss: 0.4059 - val_accuracy: 0.8683\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2709 - accuracy: 0.9170 - val_loss: 0.3512 - val_accuracy: 0.9001\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2315 - accuracy: 0.9326 - val_loss: 0.3453 - val_accuracy: 0.9112\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2047 - accuracy: 0.9469 - val_loss: 0.3197 - val_accuracy: 0.9201\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1799 - accuracy: 0.9548 - val_loss: 0.3296 - val_accuracy: 0.9157\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1709 - accuracy: 0.9548 - val_loss: 0.2997 - val_accuracy: 0.9186\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1609 - accuracy: 0.9600 - val_loss: 0.3389 - val_accuracy: 0.9209\n",
      "Max accuracy for 64px - Composante-Diversite-NucleScore: 0.9209\n",
      "Model saved to models\\64px\\Composante-Diversite-NucleScore\n",
      "\n",
      "Training on 64px/Skew-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.9302 - accuracy: 0.6868 - val_loss: 0.9550 - val_accuracy: 0.6768\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6576 - accuracy: 0.7983 - val_loss: 0.6515 - val_accuracy: 0.7848\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.5027 - accuracy: 0.8450 - val_loss: 0.5682 - val_accuracy: 0.8121\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4187 - accuracy: 0.8646 - val_loss: 0.5060 - val_accuracy: 0.8425\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.3462 - accuracy: 0.8858 - val_loss: 0.4510 - val_accuracy: 0.8735\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.2972 - accuracy: 0.9044 - val_loss: 0.4310 - val_accuracy: 0.8669\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2537 - accuracy: 0.9256 - val_loss: 0.3606 - val_accuracy: 0.9024\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.2252 - accuracy: 0.9411 - val_loss: 0.3825 - val_accuracy: 0.8817\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 14ms/step - loss: 0.2178 - accuracy: 0.9408 - val_loss: 0.3218 - val_accuracy: 0.9105\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1857 - accuracy: 0.9520 - val_loss: 0.3026 - val_accuracy: 0.9075\n",
      "Max accuracy for 64px - Skew-Composante-Diversite: 0.9105\n",
      "Model saved to models\\64px\\Skew-Composante-Diversite\n",
      "\n",
      "Training on 64px/Skew-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 3s 17ms/step - loss: 0.9381 - accuracy: 0.6772 - val_loss: 0.9137 - val_accuracy: 0.7263\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6187 - accuracy: 0.8132 - val_loss: 0.6531 - val_accuracy: 0.7862\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4666 - accuracy: 0.8533 - val_loss: 0.5607 - val_accuracy: 0.8217\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.3783 - accuracy: 0.8743 - val_loss: 0.4660 - val_accuracy: 0.8528\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 14ms/step - loss: 0.3058 - accuracy: 0.8980 - val_loss: 0.4179 - val_accuracy: 0.8757\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 3s 15ms/step - loss: 0.2557 - accuracy: 0.9260 - val_loss: 0.3762 - val_accuracy: 0.8817\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 14ms/step - loss: 0.2138 - accuracy: 0.9432 - val_loss: 0.3427 - val_accuracy: 0.8972\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1900 - accuracy: 0.9507 - val_loss: 0.3749 - val_accuracy: 0.9016\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1788 - accuracy: 0.9544 - val_loss: 0.3348 - val_accuracy: 0.9120\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1587 - accuracy: 0.9609 - val_loss: 0.3288 - val_accuracy: 0.9135\n",
      "Max accuracy for 64px - Skew-Composante-NucleScore: 0.9135\n",
      "Model saved to models\\64px\\Skew-Composante-NucleScore\n",
      "\n",
      "Training on 64px/Skew-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 18ms/step - loss: 0.8821 - accuracy: 0.7092 - val_loss: 0.8267 - val_accuracy: 0.6930\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5893 - accuracy: 0.8190 - val_loss: 0.6401 - val_accuracy: 0.7929\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5014 - accuracy: 0.8419 - val_loss: 0.5863 - val_accuracy: 0.8010\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4265 - accuracy: 0.8562 - val_loss: 0.5204 - val_accuracy: 0.8395\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3553 - accuracy: 0.8858 - val_loss: 0.4275 - val_accuracy: 0.8669\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2747 - accuracy: 0.9203 - val_loss: 0.3829 - val_accuracy: 0.8839\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2383 - accuracy: 0.9341 - val_loss: 0.3354 - val_accuracy: 0.9016\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2000 - accuracy: 0.9489 - val_loss: 0.3001 - val_accuracy: 0.9157\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1836 - accuracy: 0.9522 - val_loss: 0.2947 - val_accuracy: 0.9164\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1723 - accuracy: 0.9557 - val_loss: 0.2715 - val_accuracy: 0.9253\n",
      "Max accuracy for 64px - Skew-Diversite-NucleScore: 0.9253\n",
      "Model saved to models\\64px\\Skew-Diversite-NucleScore\n",
      "\n",
      "Training on 100px/Chargaff-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 5s 26ms/step - loss: 0.9968 - accuracy: 0.6444 - val_loss: 0.9531 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7572 - accuracy: 0.7372 - val_loss: 0.4734 - val_accuracy: 0.8669\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3620 - accuracy: 0.8802 - val_loss: 0.3901 - val_accuracy: 0.8639\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2760 - accuracy: 0.9138 - val_loss: 0.3566 - val_accuracy: 0.9016\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2174 - accuracy: 0.9387 - val_loss: 0.3431 - val_accuracy: 0.8950\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1937 - accuracy: 0.9461 - val_loss: 0.3754 - val_accuracy: 0.8891\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1781 - accuracy: 0.9522 - val_loss: 0.3598 - val_accuracy: 0.9120\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1674 - accuracy: 0.9564 - val_loss: 0.3373 - val_accuracy: 0.9135\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1584 - accuracy: 0.9576 - val_loss: 0.3976 - val_accuracy: 0.9231\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1501 - accuracy: 0.9612 - val_loss: 0.3373 - val_accuracy: 0.9201\n",
      "Max accuracy for 100px - Chargaff-Composante-Diversite: 0.9231\n",
      "Model saved to models\\100px\\Chargaff-Composante-Diversite\n",
      "\n",
      "Training on 100px/Chargaff-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 5s 26ms/step - loss: 0.9984 - accuracy: 0.6468 - val_loss: 0.9494 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.7691 - accuracy: 0.7400 - val_loss: 0.5520 - val_accuracy: 0.8528\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3879 - accuracy: 0.8773 - val_loss: 0.3715 - val_accuracy: 0.8720\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2627 - accuracy: 0.9216 - val_loss: 0.3474 - val_accuracy: 0.8964\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2182 - accuracy: 0.9360 - val_loss: 0.3133 - val_accuracy: 0.8913\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1928 - accuracy: 0.9428 - val_loss: 0.3337 - val_accuracy: 0.8950\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1753 - accuracy: 0.9517 - val_loss: 0.3329 - val_accuracy: 0.9201\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1631 - accuracy: 0.9563 - val_loss: 0.3435 - val_accuracy: 0.9135\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1584 - accuracy: 0.9588 - val_loss: 0.3272 - val_accuracy: 0.9098\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1473 - accuracy: 0.9620 - val_loss: 0.3742 - val_accuracy: 0.9179\n",
      "Max accuracy for 100px - Chargaff-Composante-NucleScore: 0.9201\n",
      "Model saved to models\\100px\\Chargaff-Composante-NucleScore\n",
      "\n",
      "Training on 100px/Chargaff-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.9638 - accuracy: 0.6518 - val_loss: 0.8640 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5781 - accuracy: 0.8310 - val_loss: 0.4859 - val_accuracy: 0.8543\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3669 - accuracy: 0.8784 - val_loss: 0.3908 - val_accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2972 - accuracy: 0.9002 - val_loss: 0.3416 - val_accuracy: 0.8950\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2470 - accuracy: 0.9280 - val_loss: 0.3178 - val_accuracy: 0.8950\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2048 - accuracy: 0.9426 - val_loss: 0.3379 - val_accuracy: 0.9009\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1882 - accuracy: 0.9494 - val_loss: 0.3112 - val_accuracy: 0.8942\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1714 - accuracy: 0.9548 - val_loss: 0.3152 - val_accuracy: 0.9201\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1594 - accuracy: 0.9587 - val_loss: 0.3117 - val_accuracy: 0.9186\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1514 - accuracy: 0.9622 - val_loss: 0.3225 - val_accuracy: 0.8972\n",
      "Max accuracy for 100px - Chargaff-Diversite-NucleScore: 0.9201\n",
      "Model saved to models\\100px\\Chargaff-Diversite-NucleScore\n",
      "\n",
      "Training on 100px/Chargaff-Skew-Composante...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.9302 - accuracy: 0.6693 - val_loss: 0.8301 - val_accuracy: 0.8121\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6652 - accuracy: 0.8070 - val_loss: 0.5373 - val_accuracy: 0.8831\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4516 - accuracy: 0.8719 - val_loss: 0.4514 - val_accuracy: 0.8846\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3704 - accuracy: 0.8775 - val_loss: 0.4029 - val_accuracy: 0.8757\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2658 - accuracy: 0.9199 - val_loss: 0.3924 - val_accuracy: 0.8854\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2212 - accuracy: 0.9349 - val_loss: 0.3902 - val_accuracy: 0.8920\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1914 - accuracy: 0.9478 - val_loss: 0.3831 - val_accuracy: 0.9009\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1753 - accuracy: 0.9522 - val_loss: 0.3672 - val_accuracy: 0.9083\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1523 - accuracy: 0.9607 - val_loss: 0.4297 - val_accuracy: 0.9031\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1495 - accuracy: 0.9609 - val_loss: 0.3801 - val_accuracy: 0.9083\n",
      "Max accuracy for 100px - Chargaff-Skew-Composante: 0.9083\n",
      "Model saved to models\\100px\\Chargaff-Skew-Composante\n",
      "\n",
      "Training on 100px/Chargaff-Skew-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.8768 - accuracy: 0.6953 - val_loss: 0.7256 - val_accuracy: 0.8321\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5333 - accuracy: 0.8538 - val_loss: 0.4726 - val_accuracy: 0.8728\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3864 - accuracy: 0.8764 - val_loss: 0.4101 - val_accuracy: 0.8735\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3061 - accuracy: 0.8996 - val_loss: 0.4162 - val_accuracy: 0.8550\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2492 - accuracy: 0.9256 - val_loss: 0.3882 - val_accuracy: 0.8757\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2183 - accuracy: 0.9365 - val_loss: 0.4165 - val_accuracy: 0.8868\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1943 - accuracy: 0.9441 - val_loss: 0.3742 - val_accuracy: 0.9046\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1753 - accuracy: 0.9526 - val_loss: 0.3700 - val_accuracy: 0.9157\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1646 - accuracy: 0.9557 - val_loss: 0.4492 - val_accuracy: 0.8987\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1561 - accuracy: 0.9583 - val_loss: 0.4125 - val_accuracy: 0.9149\n",
      "Max accuracy for 100px - Chargaff-Skew-Diversite: 0.9157\n",
      "Model saved to models\\100px\\Chargaff-Skew-Diversite\n",
      "\n",
      "Training on 100px/Chargaff-Skew-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.9185 - accuracy: 0.6804 - val_loss: 0.8848 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6211 - accuracy: 0.8223 - val_loss: 0.5062 - val_accuracy: 0.8654\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4006 - accuracy: 0.8758 - val_loss: 0.4292 - val_accuracy: 0.8654\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2946 - accuracy: 0.9044 - val_loss: 0.4053 - val_accuracy: 0.8750\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2308 - accuracy: 0.9291 - val_loss: 0.3687 - val_accuracy: 0.8876\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2039 - accuracy: 0.9382 - val_loss: 0.4308 - val_accuracy: 0.9068\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1848 - accuracy: 0.9485 - val_loss: 0.3680 - val_accuracy: 0.8935\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1678 - accuracy: 0.9522 - val_loss: 0.4000 - val_accuracy: 0.9186\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1564 - accuracy: 0.9570 - val_loss: 0.3981 - val_accuracy: 0.8964\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1550 - accuracy: 0.9583 - val_loss: 0.3964 - val_accuracy: 0.9223\n",
      "Max accuracy for 100px - Chargaff-Skew-NucleScore: 0.9223\n",
      "Model saved to models\\100px\\Chargaff-Skew-NucleScore\n",
      "\n",
      "Training on 100px/Composante-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.9921 - accuracy: 0.6451 - val_loss: 0.9344 - val_accuracy: 0.6516\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6687 - accuracy: 0.7806 - val_loss: 0.4524 - val_accuracy: 0.8683\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3746 - accuracy: 0.8788 - val_loss: 0.5294 - val_accuracy: 0.8484\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3182 - accuracy: 0.8909 - val_loss: 0.3629 - val_accuracy: 0.8824\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2610 - accuracy: 0.9223 - val_loss: 0.3369 - val_accuracy: 0.9031\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2174 - accuracy: 0.9385 - val_loss: 0.3139 - val_accuracy: 0.8979\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1916 - accuracy: 0.9469 - val_loss: 0.3419 - val_accuracy: 0.9127\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1757 - accuracy: 0.9553 - val_loss: 0.3117 - val_accuracy: 0.9098\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1606 - accuracy: 0.9557 - val_loss: 0.3198 - val_accuracy: 0.9105\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1491 - accuracy: 0.9625 - val_loss: 0.3229 - val_accuracy: 0.9231\n",
      "Max accuracy for 100px - Composante-Diversite-NucleScore: 0.9231\n",
      "Model saved to models\\100px\\Composante-Diversite-NucleScore\n",
      "\n",
      "Training on 100px/Skew-Composante-Diversite...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.8844 - accuracy: 0.6940 - val_loss: 0.7372 - val_accuracy: 0.7441\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5372 - accuracy: 0.8524 - val_loss: 0.5138 - val_accuracy: 0.8794\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3982 - accuracy: 0.8754 - val_loss: 0.4726 - val_accuracy: 0.8454\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3025 - accuracy: 0.8994 - val_loss: 0.4780 - val_accuracy: 0.8476\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2578 - accuracy: 0.9218 - val_loss: 0.3767 - val_accuracy: 0.8846\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.2173 - accuracy: 0.9343 - val_loss: 0.4420 - val_accuracy: 0.8676\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1953 - accuracy: 0.9432 - val_loss: 0.4018 - val_accuracy: 0.9031\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1781 - accuracy: 0.9483 - val_loss: 0.4499 - val_accuracy: 0.9038\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 13ms/step - loss: 0.1696 - accuracy: 0.9550 - val_loss: 0.3971 - val_accuracy: 0.9075\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1563 - accuracy: 0.9577 - val_loss: 0.4075 - val_accuracy: 0.9238\n",
      "Max accuracy for 100px - Skew-Composante-Diversite: 0.9238\n",
      "Model saved to models\\100px\\Skew-Composante-Diversite\n",
      "\n",
      "Training on 100px/Skew-Composante-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 22ms/step - loss: 0.9231 - accuracy: 0.6763 - val_loss: 0.7729 - val_accuracy: 0.7729\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.5618 - accuracy: 0.8465 - val_loss: 0.5578 - val_accuracy: 0.8536\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3742 - accuracy: 0.8808 - val_loss: 0.4035 - val_accuracy: 0.8698\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2702 - accuracy: 0.9151 - val_loss: 0.4029 - val_accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2209 - accuracy: 0.9347 - val_loss: 0.3840 - val_accuracy: 0.8780\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1871 - accuracy: 0.9443 - val_loss: 0.3991 - val_accuracy: 0.9061\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1761 - accuracy: 0.9524 - val_loss: 0.3535 - val_accuracy: 0.9038\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1606 - accuracy: 0.9533 - val_loss: 0.4579 - val_accuracy: 0.8491\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1534 - accuracy: 0.9579 - val_loss: 0.3747 - val_accuracy: 0.9209\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1420 - accuracy: 0.9601 - val_loss: 0.3729 - val_accuracy: 0.9142\n",
      "Max accuracy for 100px - Skew-Composante-NucleScore: 0.9209\n",
      "Model saved to models\\100px\\Skew-Composante-NucleScore\n",
      "\n",
      "Training on 100px/Skew-Diversite-NucleScore...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 4s 21ms/step - loss: 0.9166 - accuracy: 0.6715 - val_loss: 0.8486 - val_accuracy: 0.6820\n",
      "Epoch 2/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.6256 - accuracy: 0.8164 - val_loss: 0.5055 - val_accuracy: 0.8780\n",
      "Epoch 3/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.4366 - accuracy: 0.8705 - val_loss: 0.4477 - val_accuracy: 0.8824\n",
      "Epoch 4/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.3573 - accuracy: 0.8804 - val_loss: 0.4723 - val_accuracy: 0.8669\n",
      "Epoch 5/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2865 - accuracy: 0.9068 - val_loss: 0.3788 - val_accuracy: 0.8750\n",
      "Epoch 6/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2390 - accuracy: 0.9278 - val_loss: 0.3870 - val_accuracy: 0.8935\n",
      "Epoch 7/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.2018 - accuracy: 0.9389 - val_loss: 0.3992 - val_accuracy: 0.9083\n",
      "Epoch 8/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1844 - accuracy: 0.9470 - val_loss: 0.4026 - val_accuracy: 0.9142\n",
      "Epoch 9/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1679 - accuracy: 0.9517 - val_loss: 0.4285 - val_accuracy: 0.8979\n",
      "Epoch 10/10\n",
      "170/170 [==============================] - 2s 12ms/step - loss: 0.1579 - accuracy: 0.9566 - val_loss: 0.4439 - val_accuracy: 0.9179\n",
      "Max accuracy for 100px - Skew-Diversite-NucleScore: 0.9179\n",
      "Model saved to models\\100px\\Skew-Diversite-NucleScore\n"
     ]
    }
   ],
   "source": [
    "# Iterate through resolution folders\n",
    "for resolution_folder in sorted(os.listdir(images_dir), key=lambda x: int(re.search(r'\\d+', x).group())):\n",
    "\t# sort resolution folders to ensure consistent order\n",
    "\tresolution_path = os.path.join(images_dir, resolution_folder)\n",
    "\n",
    "\tif os.path.isdir(resolution_path):\n",
    "\t\t# Iterate through method folders inside resolution\n",
    "\t\tfor method_folder in os.listdir(resolution_path):\n",
    "\t\t\tmethod_path = os.path.join(resolution_path, method_folder)\n",
    "\n",
    "\t\t\tif os.path.isdir(method_path):\n",
    "\t\t\t\tprint(f\"\\nTraining on {resolution_folder}/{method_folder}...\")\n",
    "\n",
    "\t\t\t\t# Data Preprocessing\n",
    "\t\t\t\tdatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\t\t\t\ttrain_data = datagen.flow_from_directory(\n",
    "\t\t\t\t\tmethod_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\t\t\tclass_mode='categorical', subset='training'\n",
    "\t\t\t\t)\n",
    "\t\t\t\tval_data = datagen.flow_from_directory(\n",
    "\t\t\t\t\tmethod_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\t\t\tclass_mode='categorical', subset='validation'\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t\t# Build and train CNN model\n",
    "\t\t\t\tmodel = cnn_model(input_shape=(img_size[0], img_size[1], 3))\n",
    "\t\t\t\thistory = model.fit(train_data, validation_data=val_data, epochs=epochs, verbose=1)\n",
    "\n",
    "\t\t\t\t# Get max validation accuracy\n",
    "\t\t\t\tmax_acc = max(history.history['val_accuracy'])\n",
    "\t\t\t\tkey = f\"{resolution_folder} - {method_folder}\"\n",
    "\t\t\t\tmax_accuracies[key] = max_acc\n",
    "\t\t\t\tprint(f\"Max accuracy for {key}: {max_acc:.4f}\")\n",
    "\n",
    "\t\t\t\t# Save model\n",
    "\t\t\t\tmodel_dir = os.path.join(\"models\", resolution_folder, method_folder)\n",
    "\t\t\t\tos.makedirs(model_dir, exist_ok=True)\n",
    "\t\t\t\tmodel_name = f\"{resolution_folder}_{method_folder}.h5\"\n",
    "\t\t\t\tmodel.save(os.path.join(model_dir, model_name))\n",
    "\t\t\t\tprint(f\"Model saved to {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\3164836006.py:21: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Create positions for bars\n",
    "num_bars = len(max_accuracies)\n",
    "x_positions = np.arange(num_bars)\n",
    "\n",
    "# Add extra space every 10 bars\n",
    "for i in range(10, num_bars, 10):  \n",
    "\tx_positions[i:] += 1  # Shift everything after every 10th bar\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(x_positions, max_accuracies.values(), color='blue', width=0.5)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Custom x-ticks for readability\n",
    "plt.xticks(x_positions, max_accuracies.keys(), rotation=90)\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "\n",
    "plt.ylabel(\"Max Accuracy\")\n",
    "plt.title(\"CNN Validation Results\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\2751164684.py:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap(\"tab10\", len(methods))  # Use \"tab10\" color map with enough colors\n",
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\2751164684.py:31: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Extract and sort resolutions numerically\n",
    "resolutions = sorted(set(k.split(\" - \")[0] for k in max_accuracies.keys()), key=lambda x: int(x.replace(\"px\", \"\")))\n",
    "methods = sorted(set(k.split(\" - \")[1] for k in max_accuracies.keys()))\n",
    "\n",
    "# Organize data for plotting\n",
    "data = {method: [max_accuracies.get(f\"{res} - {method}\", None) for res in resolutions] for method in methods}\n",
    "\n",
    "# Define a color map for better distinction\n",
    "colors = plt.cm.get_cmap(\"tab10\", len(methods))  # Use \"tab10\" color map with enough colors\n",
    "\n",
    "# Plot the lines\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, (method, accuracies) in enumerate(data.items()):\n",
    "\tplt.plot(resolutions, accuracies, marker=\"o\", linestyle=\"-\", linewidth=2, markersize=8, label=method, color=colors(i))\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Resolution\", fontsize=12)\n",
    "plt.ylabel(\"Max Accuracy\", fontsize=12)\n",
    "plt.title(\"CNN Validation Accuracy by Resolution and Method\", fontsize=14)\n",
    "\n",
    "# Move legend outside the plot for better clarity\n",
    "plt.legend(title=\"Method\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=10)\n",
    "\n",
    "# Add a grid with transparency\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Improve layout to fit legend properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/cnn_validation_accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\4277257368.py:16: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap(\"tab10\", len(methods_group1))  # Use \"tab10\" color map with enough colors\n",
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\4277257368.py:37: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Extract and sort resolutions numerically\n",
    "resolutions = sorted(set(k.split(\" - \")[0] for k in max_accuracies.keys()), key=lambda x: int(x.replace(\"px\", \"\")))\n",
    "\n",
    "# Extract unique methods and split into two groups of 5\n",
    "methods = sorted(set(k.split(\" - \")[1] for k in max_accuracies.keys()))\n",
    "methods_group1 = methods[:5]\n",
    "methods_group2 = methods[5:]\n",
    "\n",
    "# Organize data for plotting\n",
    "data1 = {method: [max_accuracies.get(f\"{res} - {method}\", None) for res in resolutions] for method in methods_group1}\n",
    "data2 = {method: [max_accuracies.get(f\"{res} - {method}\", None) for res in resolutions] for method in methods_group2}\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "colors = plt.cm.get_cmap(\"tab10\", len(methods_group1))  # Use \"tab10\" color map with enough colors\n",
    "\n",
    "# Plot first group\n",
    "for method, accuracies in data1.items():\n",
    "\taxs[0].plot(resolutions, accuracies, marker=\"o\", label=method, color=colors(methods_group1.index(method)))\n",
    "axs[0].set_title(\"CNN Validation Accuracy (Group 1)\")\n",
    "axs[0].set_ylabel(\"Max Accuracy\")\n",
    "axs[0].legend(title=\"Method\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot second group\n",
    "for method, accuracies in data2.items():\n",
    "\taxs[1].plot(resolutions, accuracies, marker=\"o\", label=method, color=colors(methods_group2.index(method)))\n",
    "axs[1].set_title(\"CNN Validation Accuracy (Group 2)\")\n",
    "axs[1].set_xlabel(\"Resolution\")\n",
    "axs[1].set_ylabel(\"Max Accuracy\")\n",
    "axs[1].legend(title=\"Method\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/cnn_validation_accuracy_groups.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_192 (Conv2D)         (None, 8, 8, 32)          896       \n",
      "                                                                 \n",
      " max_pooling2d_192 (MaxPool  (None, 4, 4, 32)          0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " conv2d_193 (Conv2D)         (None, 2, 2, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_193 (MaxPool  (None, 1, 1, 64)          0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " flatten_96 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_192 (Dense)           (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_193 (Dense)           (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28357 (110.77 KB)\n",
      "Trainable params: 28357 (110.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Found 6771 images belonging to 5 classes.\n",
      "212/212 [==============================] - 2s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\4260248153.py:38: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the model\n",
    "model_path = \"models/100px/Chargaff-Composante-Diversite/100px_Chargaff-Composante-Diversite.h5\"\n",
    "model = load_model(model_path)\n",
    "model.summary()\n",
    "\n",
    "# Load the test data\n",
    "test_data_dir = 'images/100px/Chargaff-Composante-Diversite'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "\ttest_data_dir,\n",
    "\ttarget_size=(img_size[0], img_size[1]),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='categorical',\n",
    "\tshuffle=False\n",
    ")\n",
    "# Get the true labels\n",
    "true_labels = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Get the predicted labels\n",
    "predictions = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/cnn_confusion_matrix_100px.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "mosaics_dir = \"C:/Users/theof/OneDrive/Documents/Github/genome_color_unpickler/res\"\n",
    "\n",
    "# Training parameters\n",
    "img_size = (20, 50)  # Resize images to 64x64\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Dictionary to store max accuracies\n",
    "max_accuracies_mosaics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 4px...\n",
      "Found 5419 images belonging to 5 classes.\n",
      "Found 1352 images belonging to 5 classes.\n",
      "Epoch 1/10\n",
      "170/170 [==============================] - 5s 23ms/step - loss: 0.9636 - accuracy: 0.6507 - val_loss: 0.9474 - val_accuracy: 0.6376\n",
      "Epoch 2/10\n",
      "167/170 [============================>.] - ETA: 0s - loss: 0.9011 - accuracy: 0.6788"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Build and train CNN model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m cnn_model(input_shape\u001b[38;5;241m=\u001b[39m(img_size[\u001b[38;5;241m0\u001b[39m], img_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 22\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Get max validation accuracy\u001b[39;00m\n\u001b[0;32m     25\u001b[0m max_acc_mos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1832\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1818\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1819\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1830\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1831\u001b[0m     )\n\u001b[1;32m-> 1832\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1841\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1847\u001b[0m }\n\u001b[0;32m   1848\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:2272\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   2269\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2270\u001b[0m             ):\n\u001b[0;32m   2271\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 2272\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2276\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2277\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2279\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2280\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:4079\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[1;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[0;32m   4078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[1;32m-> 4079\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   4081\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:876\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 876\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    880\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\theof\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# mosaic version\n",
    "# Iterate through resolution folders\n",
    "for resolution_folder in sorted(os.listdir(images_dir), key=lambda x: int(re.search(r'\\d+', x).group())):\n",
    "\tresolution_path = os.path.join(mosaics_dir, resolution_folder)\n",
    "\n",
    "\tif os.path.isdir(resolution_path):\n",
    "\t\tprint(f\"\\nTraining on {resolution_folder}...\")\n",
    "\n",
    "\t\t# Data Preprocessing\n",
    "\t\tdatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\t\ttrain_data = datagen.flow_from_directory(\n",
    "\t\t\tresolution_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\tclass_mode='categorical', subset='training'\n",
    "\t\t)\n",
    "\t\tval_data = datagen.flow_from_directory(\n",
    "\t\t\tresolution_path, target_size=img_size, batch_size=batch_size,\n",
    "\t\t\tclass_mode='categorical', subset='validation'\n",
    "\t\t)\n",
    "\n",
    "\t\t# Build and train CNN model\n",
    "\t\tmodel = cnn_model(input_shape=(img_size[0], img_size[1], 3))\n",
    "\t\thistory = model.fit(train_data, validation_data=val_data, epochs=epochs, verbose=1)\n",
    "\n",
    "\t\t# Get max validation accuracy\n",
    "\t\tmax_acc_mos = max(history.history['val_accuracy'])\n",
    "\t\tmax_accuracies_mosaics[resolution_folder] = max_acc_mos\n",
    "\t\tprint(f\"Max accuracy for {resolution_folder}: {max_acc_mos:.4f}\")\n",
    "\n",
    "\t\t# Save model\n",
    "\t\t# model_dir = os.path.join(\"models/mosaics\", resolution_folder)\n",
    "\t\t# os.makedirs(model_dir, exist_ok=True)\n",
    "\t\t# model_name = f\"{resolution_folder}_mosaic.h5\"\n",
    "\t\t# model.save(os.path.join(model_dir, model_name))\n",
    "\t\t# print(f\"Model saved to {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\2020362206.py:21: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Create positions for bars\n",
    "num_bars = len(max_accuracies_mosaics)\n",
    "x_positions = np.arange(num_bars)\n",
    "\n",
    "# Add extra space every 10 bars\n",
    "for i in range(10, num_bars, 10):  \n",
    "\tx_positions[i:] += 1  # Shift everything after every 10th bar\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.bar(x_positions, max_accuracies_mosaics.values(), color='blue', width=0.5)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Custom x-ticks for readability\n",
    "plt.xticks(x_positions, max_accuracies_mosaics.keys(), rotation=90)\n",
    "\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "\n",
    "plt.ylabel(\"Max Accuracy\")\n",
    "plt.title(\"CNN Validation Results\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/cnn_validation_accuracy_mosaics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\331704733.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Extract and sort resolutions numerically\n",
    "resolutions = sorted(max_accuracies_mosaics.keys(), key=lambda x: int(x.replace(\"px\", \"\")))\n",
    "\n",
    "# Get max accuracy values in the correct order\n",
    "accuracies = [max_accuracies_mosaics[res] for res in resolutions]\n",
    "\n",
    "# Plot the accuracy per resolution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(resolutions, accuracies, marker=\"o\", linestyle=\"-\", linewidth=2, markersize=8, color=\"blue\", label=\"Accuracy\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Resolution\", fontsize=12)\n",
    "plt.ylabel(\"Max Accuracy\", fontsize=12)\n",
    "plt.title(\"CNN Validation Accuracy by Resolution\", fontsize=14)\n",
    "\n",
    "# Add data points on the plot\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(resolutions[i], acc, f\"{acc:.2f}\", fontsize=10, ha=\"right\")\n",
    "\n",
    "# Grid and legend\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"../../imgs/graphs/cnn_validation_accuracy_mosaics_line.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_110 (Conv2D)         (None, 18, 48, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_110 (MaxPool  (None, 9, 24, 32)         0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " conv2d_111 (Conv2D)         (None, 7, 22, 64)         18496     \n",
      "                                                                 \n",
      " max_pooling2d_111 (MaxPool  (None, 3, 11, 64)         0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " flatten_55 (Flatten)        (None, 2112)              0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 128)               270464    \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 290501 (1.11 MB)\n",
      "Trainable params: 290501 (1.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Found 6771 images belonging to 5 classes.\n",
      "212/212 [==============================] - 3s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theof\\AppData\\Local\\Temp\\ipykernel_27196\\3084799137.py:35: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Load the model for the new version\n",
    "new_model_path = \"models/mosaics/100px/100px_mosaic.h5\"\n",
    "new_model = load_model(new_model_path)\n",
    "new_model.summary()\n",
    "\n",
    "# Load the test data for the new version\n",
    "new_test_data_dir = 'C:/Users/theof/OneDrive/Documents/Github/genome_color_unpickler/res/100px'\n",
    "\n",
    "new_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "new_test_generator = new_test_datagen.flow_from_directory(\n",
    "\tnew_test_data_dir,\n",
    "\ttarget_size=(img_size[0], img_size[1]),\n",
    "\tbatch_size=batch_size,\n",
    "\tclass_mode='categorical',\n",
    "\tshuffle=False\n",
    ")\n",
    "\n",
    "# Get the true labels for the new version\n",
    "new_true_labels = new_test_generator.classes\n",
    "new_class_labels = list(new_test_generator.class_indices.keys())\n",
    "\n",
    "# Get the predicted labels for the new version\n",
    "new_predictions = new_model.predict(new_test_generator, steps=len(new_test_generator), verbose=1)\n",
    "new_predicted_labels = np.argmax(new_predictions, axis=1)\n",
    "\n",
    "# Generate the confusion matrix for the new version\n",
    "new_cm = confusion_matrix(new_true_labels, new_predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix for the new version\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(new_cm, annot=True, fmt='d', cmap='Blues', xticklabels=new_class_labels, yticklabels=new_class_labels)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix for New Version')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(\"../../imgs/graphs/cnn_confusion_matrix_mosaics_100px.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
